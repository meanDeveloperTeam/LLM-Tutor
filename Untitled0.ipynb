{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPognhHCA/Dg00/5GmzyzwI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meanDeveloperTeam/LLM-Tutor/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqKUWSmcd7iy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "506fd389"
      },
      "source": [
        "# Task\n",
        "Create a friendly AI interviewer that can conduct mock interviews and provide training for Mean Stack, MERN Stack, and AI roles to help users prepare for MNC interviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37f176de"
      },
      "source": [
        "## Set up the environment\n",
        "\n",
        "### Subtask:\n",
        "Install the necessary libraries and configure the environment for using large language models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98ac053f"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the necessary libraries for interacting with large language models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5f0845f",
        "outputId": "54842341-21a8-4071-c740-03af59ece7fa"
      },
      "source": [
        "%pip install transformers openai google-generativeai"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.96.1)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.176.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fce0f298"
      },
      "source": [
        "**Reasoning**:\n",
        "Configure the environment for accessing the large language models by setting environment variables for API keys.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ebc7a5e"
      },
      "source": [
        "import os\n",
        "\n",
        "# Replace with your actual API keys or configuration details\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\" # Replace with your OpenAI key if needed\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCJpR6GjKUoMfd6F2d3FC7u8IxvN0wyZi0\"\n",
        "\n",
        "# You might also need to set up authentication for other models or local models\n",
        "# For example, for Hugging Face transformers, you might need to log in\n",
        "# from huggingface_hub import login\n",
        "# login()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00f9f3d2"
      },
      "source": [
        "## Define the interviewer persona\n",
        "\n",
        "### Subtask:\n",
        "Define the characteristics and style of the friendly interviewer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1755fa74"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the characteristics and style of the AI interviewer based on the instructions. I will create a text description outlining these aspects.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57bb1024",
        "outputId": "2b5d5cc4-1686-4cd9-b6bc-72da1fe10d4b"
      },
      "source": [
        "interviewer_personality = \"\"\"\n",
        "The AI interviewer should be:\n",
        "- Friendly and approachable: Creating a comfortable and low-stress environment for the user.\n",
        "- Encouraging: Providing positive reinforcement and constructive feedback to boost confidence.\n",
        "- Knowledgeable: Possessing a strong understanding of Mean Stack, MERN Stack, and AI roles and relevant interview topics.\n",
        "- Patient: Allowing users time to think and respond, and being willing to rephrase questions if needed.\n",
        "- Adaptable: Adjusting the difficulty and focus of questions based on the user's responses and experience level.\n",
        "\"\"\"\n",
        "\n",
        "interviewer_tone_style = \"\"\"\n",
        "The communication style should be:\n",
        "- Conversational: Engaging in a natural and flowing dialogue rather than just asking a list of questions.\n",
        "- Professional: Maintaining a level of formality appropriate for a mock interview setting.\n",
        "- Clear and concise: Asking questions and providing feedback in an easy-to-understand manner.\n",
        "- Supportive: Framing feedback in a way that helps the user learn and improve.\n",
        "\"\"\"\n",
        "\n",
        "positive_interview_traits = \"\"\"\n",
        "Traits and behaviors that contribute to a positive experience include:\n",
        "- Providing clear instructions at the beginning of the interview.\n",
        "- Offering specific and actionable feedback after each question or at the end of the interview.\n",
        "- Allowing the user to ask questions about the role or company (simulated).\n",
        "- Ending the interview with encouragement and next steps for improvement.\n",
        "- Maintaining a positive and optimistic demeanor throughout the interaction.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Desired Interviewer Personality:\")\n",
        "print(interviewer_personality)\n",
        "print(\"\\nInterviewer Tone and Style:\")\n",
        "print(interviewer_tone_style)\n",
        "print(\"\\nPositive Interview Traits:\")\n",
        "print(positive_interview_traits)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Desired Interviewer Personality:\n",
            "\n",
            "The AI interviewer should be:\n",
            "- Friendly and approachable: Creating a comfortable and low-stress environment for the user.\n",
            "- Encouraging: Providing positive reinforcement and constructive feedback to boost confidence.\n",
            "- Knowledgeable: Possessing a strong understanding of Mean Stack, MERN Stack, and AI roles and relevant interview topics.\n",
            "- Patient: Allowing users time to think and respond, and being willing to rephrase questions if needed.\n",
            "- Adaptable: Adjusting the difficulty and focus of questions based on the user's responses and experience level.\n",
            "\n",
            "\n",
            "Interviewer Tone and Style:\n",
            "\n",
            "The communication style should be:\n",
            "- Conversational: Engaging in a natural and flowing dialogue rather than just asking a list of questions.\n",
            "- Professional: Maintaining a level of formality appropriate for a mock interview setting.\n",
            "- Clear and concise: Asking questions and providing feedback in an easy-to-understand manner.\n",
            "- Supportive: Framing feedback in a way that helps the user learn and improve.\n",
            "\n",
            "\n",
            "Positive Interview Traits:\n",
            "\n",
            "Traits and behaviors that contribute to a positive experience include:\n",
            "- Providing clear instructions at the beginning of the interview.\n",
            "- Offering specific and actionable feedback after each question or at the end of the interview.\n",
            "- Allowing the user to ask questions about the role or company (simulated).\n",
            "- Ending the interview with encouragement and next steps for improvement.\n",
            "- Maintaining a positive and optimistic demeanor throughout the interaction.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a4e7dc4"
      },
      "source": [
        "## Generate interview questions\n",
        "\n",
        "### Subtask:\n",
        "Generate relevant interview questions for Mean Stack, MERN Stack, and AI roles.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73aa4636"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function to generate interview questions using a language model based on the specified role.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "1e35da28",
        "outputId": "6201d0ab-7218-4f6d-bc07-e4240382a0a8"
      },
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Configure the generative AI model (using Google's model as an example)\n",
        "# Ensure you have set the GOOGLE_API_KEY environment variable\n",
        "try:\n",
        "    # Explicitly get the API key from the environment variable here\n",
        "    api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "    if not api_key or api_key == \"YOUR_GOOGLE_API_KEY\":\n",
        "        raise ValueError(\"GOOGLE_API_KEY environment variable not set or is a placeholder.\")\n",
        "\n",
        "    genai.configure(api_key=api_key)\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash') # You can choose a different model if preferred\n",
        "    print(\"Google Generative AI configured successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to configure Google Generative AI: {e}\")\n",
        "    model = None # Set model to None if configuration fails\n",
        "\n",
        "\n",
        "def generate_interview_questions(role, model):\n",
        "    \"\"\"Generates a list of interview questions for a given role using a language model.\"\"\"\n",
        "    if model is None:\n",
        "        print(f\"Model not configured, cannot generate questions for {role}.\")\n",
        "        return []\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    As a friendly and knowledgeable AI interviewer preparing a candidate for an MNC interview, generate a list of relevant interview questions for a {role} role.\n",
        "    Include a mix of theoretical and practical questions, covering core concepts, technologies, problem-solving, and behavioral aspects.\n",
        "    Structure the output as a numbered list of questions.\n",
        "    Make sure the questions are challenging but fair for someone applying to an MNC.\n",
        "    Provide at least 10 questions for each role.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        # Assuming the response is text and can be split into a list of questions\n",
        "        # This splitting might need adjustment based on the model's output format\n",
        "        questions = response.text.strip().split('\\n')\n",
        "        # Filter out any empty lines or non-question text that might result from splitting\n",
        "        # Keep lines that start with a number followed by a period and space/parenthesis\n",
        "        questions = [q for q in questions if q.strip() and (q.strip().startswith(tuple(str(i) + '.' for i in range(10))) or q.strip().startswith(tuple(str(i) + ')' for i in range(10))))]\n",
        "        return questions\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while generating questions for {role}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Generate questions for each role if the model was configured successfully\n",
        "mean_stack_questions = []\n",
        "mern_stack_questions = []\n",
        "ai_questions = []\n",
        "\n",
        "if model:\n",
        "    mean_stack_questions = generate_interview_questions(\"Mean Stack\", model)\n",
        "    mern_stack_questions = generate_interview_questions(\"MERN Stack\", model)\n",
        "    ai_questions = generate_interview_questions(\"AI\", model)\n",
        "else:\n",
        "    print(\"Skipping question generation as the model was not configured.\")\n",
        "\n",
        "# Display the generated questions\n",
        "print(\"\\nMean Stack Interview Questions:\")\n",
        "if mean_stack_questions:\n",
        "    for q in mean_stack_questions:\n",
        "        print(q)\n",
        "else:\n",
        "    print(\"Could not generate Mean Stack questions.\")\n",
        "\n",
        "\n",
        "print(\"\\nMERN Stack Interview Questions:\")\n",
        "if mern_stack_questions:\n",
        "    for q in mern_stack_questions:\n",
        "        print(q)\n",
        "else:\n",
        "    print(\"Could not generate MERN Stack questions.\")\n",
        "\n",
        "\n",
        "print(\"\\nAI Interview Questions:\")\n",
        "if ai_questions:\n",
        "    for q in ai_questions:\n",
        "        print(q)\n",
        "else:\n",
        "    print(\"Could not generate AI questions.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Generative AI configured successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 5382.19ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while generating questions for Mean Stack: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1960.56ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while generating questions for MERN Stack: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "An error occurred while generating questions for AI: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "\n",
            "Mean Stack Interview Questions:\n",
            "Could not generate Mean Stack questions.\n",
            "\n",
            "MERN Stack Interview Questions:\n",
            "Could not generate MERN Stack questions.\n",
            "\n",
            "AI Interview Questions:\n",
            "Could not generate AI questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1635.03ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6475da10"
      },
      "source": [
        "## Generate interview questions retry 1\n",
        "\n",
        "### Subtask:\n",
        "Generate relevant interview questions for Mean Stack, MERN Stack, and AI roles.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caec998b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to generate questions failed due to an invalid API key. The environment variable `GOOGLE_API_KEY` needs to be set correctly before calling the `generate_interview_questions` function again. The instructions require ensuring the environment variable is set. Since the previous attempt failed and the error indicates an API key issue, the most logical next step is to re-attempt setting the environment variable, assuming the user has now updated it with a valid key. Then, the code to generate and print the questions for each role can be executed again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "ded82d23",
        "outputId": "1550a550-d3cf-485c-bd21-9e2da0a80074"
      },
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Ensure the environment variable is set. This is a re-attempt from a previous step.\n",
        "# In a real scenario, the user should ensure this is set correctly outside this script\n",
        "# or in the 'Set up the environment' subtask with their actual key.\n",
        "# For this re-attempt, we assume the environment is now correctly configured.\n",
        "# If running in a notebook, you might need to restart the kernel and set the key\n",
        "# before executing this cell if it wasn't set correctly before.\n",
        "\n",
        "# Configure the generative AI model\n",
        "try:\n",
        "    genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash') # Or choose a different model\n",
        "    print(\"Google Generative AI configured successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to configure Google Generative AI: {e}\")\n",
        "    model = None # Set model to None if configuration fails\n",
        "\n",
        "def generate_interview_questions(role, model):\n",
        "    \"\"\"Generates a list of interview questions for a given role using a language model.\"\"\"\n",
        "    if model is None:\n",
        "        print(f\"Model not configured, cannot generate questions for {role}.\")\n",
        "        return []\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    As a friendly and knowledgeable AI interviewer preparing a candidate for an MNC interview, generate a list of relevant interview questions for a {role} role.\n",
        "    Include a mix of theoretical and practical questions, covering core concepts, technologies, problem-solving, and behavioral aspects.\n",
        "    Structure the output as a numbered list of questions.\n",
        "    Make sure the questions are challenging but fair for someone applying to an MNC.\n",
        "    Provide at least 10 questions for each role.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        # Assuming the response is text and can be split into a list of questions\n",
        "        # This splitting might need adjustment based on the model's output format\n",
        "        questions = response.text.strip().split('\\n')\n",
        "        # Filter out any empty lines or non-question text that might result from splitting\n",
        "        # Keep lines that start with a number followed by a period and space/parenthesis\n",
        "        questions = [q for q in questions if q.strip() and (q.strip().startswith(tuple(str(i) + '.' for i in range(10))) or q.strip().startswith(tuple(str(i) + ')' for i in range(10))))]\n",
        "        return questions\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while generating questions for {role}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Generate questions for each role if the model was configured successfully\n",
        "mean_stack_questions = []\n",
        "mern_stack_questions = []\n",
        "ai_questions = []\n",
        "\n",
        "if model:\n",
        "    mean_stack_questions = generate_interview_questions(\"Mean Stack\", model)\n",
        "    mern_stack_questions = generate_interview_questions(\"MERN Stack\", model)\n",
        "    ai_questions = generate_interview_questions(\"AI\", model)\n",
        "else:\n",
        "    print(\"Skipping question generation as the model was not configured.\")\n",
        "\n",
        "\n",
        "# Print the generated questions\n",
        "print(\"\\nMean Stack Interview Questions:\")\n",
        "if mean_stack_questions:\n",
        "    for q in mean_stack_questions:\n",
        "        print(q)\n",
        "else:\n",
        "    print(\"Could not generate Mean Stack questions.\")\n",
        "\n",
        "\n",
        "print(\"\\nMERN Stack Interview Questions:\")\n",
        "if mern_stack_questions:\n",
        "    for q in mern_stack_questions:\n",
        "        print(q)\n",
        "else:\n",
        "    print(\"Could not generate MERN Stack questions.\")\n",
        "\n",
        "\n",
        "print(\"\\nAI Interview Questions:\")\n",
        "if ai_questions:\n",
        "    for q in ai_questions:\n",
        "        print(q)\n",
        "else:\n",
        "    print(\"Could not generate AI questions.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Generative AI configured successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 8348.94ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1081.76ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1860.66ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 24013.24ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2163.10ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 5408.18ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1208.47ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1057.76ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1031.17ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while generating questions for Mean Stack: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 6137.90ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1462.06ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while generating questions for MERN Stack: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "An error occurred while generating questions for AI: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "\n",
            "Mean Stack Interview Questions:\n",
            "Could not generate Mean Stack questions.\n",
            "\n",
            "MERN Stack Interview Questions:\n",
            "Could not generate MERN Stack questions.\n",
            "\n",
            "AI Interview Questions:\n",
            "Could not generate AI questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2165.39ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2340a27c"
      },
      "source": [
        "## Simulate an interview\n",
        "\n",
        "### Subtask:\n",
        "Create a flow to simulate an interview where the AI asks questions and the user responds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d15db4b"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `conduct_interview` function as described in the instructions to simulate the interview flow.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "548ef879",
        "outputId": "6bdbd6ae-4f9a-43b4-9066-547eba7a6274"
      },
      "source": [
        "def conduct_interview(role, questions):\n",
        "    \"\"\"\n",
        "    Simulates an interview flow for a given role using a list of questions.\n",
        "\n",
        "    Args:\n",
        "        role: The role for the interview (e.g., \"Mean Stack\", \"MERN Stack\", \"AI\").\n",
        "        questions: A list of interview questions for the role.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, where each dictionary contains a question and the user's response.\n",
        "    \"\"\"\n",
        "    print(f\"Starting mock interview for {role} role.\")\n",
        "    print(\"Type 'quit' at any time to end the interview.\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    interview_responses = []\n",
        "    for i, question in enumerate(questions):\n",
        "        print(f\"\\nQuestion {i+1}: {question}\")\n",
        "        user_answer = input(\"Your answer: \")\n",
        "\n",
        "        if user_answer.lower() == 'quit':\n",
        "            print(\"\\nEnding interview early.\")\n",
        "            break\n",
        "\n",
        "        interview_responses.append({\n",
        "            \"question\": question,\n",
        "            \"answer\": user_answer\n",
        "        })\n",
        "\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"Mock interview for {role} role finished.\")\n",
        "    return interview_responses\n",
        "\n",
        "# Example usage (assuming mean_stack_questions, mern_stack_questions, ai_questions are defined)\n",
        "# You would call this function after successfully generating the questions.\n",
        "# For demonstration purposes, we'll use a placeholder list of questions.\n",
        "# Note: This part is for demonstration and will be removed when integrating with question generation.\n",
        "if 'mean_stack_questions' not in locals() or not mean_stack_questions:\n",
        "    print(\"Using placeholder questions for demonstration as actual questions were not generated.\")\n",
        "    placeholder_questions = [\n",
        "        \"1. Tell me about your experience with JavaScript.\",\n",
        "        \"2. Explain the difference between synchronous and asynchronous operations in Node.js.\",\n",
        "        \"3. Describe a challenging project you worked on and how you overcame difficulties.\"\n",
        "    ]\n",
        "    # Replace with actual generated questions when available\n",
        "    # mean_stack_questions = placeholder_questions\n",
        "    # mern_stack_responses = conduct_interview(\"MERN Stack\", placeholder_questions)\n",
        "    # ai_responses = conduct_interview(\"AI\", placeholder_questions)\n",
        "\n",
        "# To avoid running the interview automatically when the cell is executed,\n",
        "# the actual function calls are commented out.\n",
        "# You would uncomment and run the desired interview after question generation is successful.\n",
        "\n",
        "# Example of how you would call it:\n",
        "# if mean_stack_questions:\n",
        "#     mean_stack_responses = conduct_interview(\"Mean Stack\", mean_stack_questions)\n",
        "#     print(\"\\nMean Stack Interview Responses:\")\n",
        "#     for response in mean_stack_responses:\n",
        "#         print(f\"Q: {response['question']}\")\n",
        "#         print(f\"A: {response['answer']}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using placeholder questions for demonstration as actual questions were not generated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bde4d1f5"
      },
      "source": [
        "## Evaluate responses\n",
        "\n",
        "### Subtask:\n",
        "Develop a method to evaluate the user's responses based on correctness, clarity, and completeness.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28aeefad"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `evaluate_answer` function as described in the instructions, using a language model to assess the user's response based on correctness, clarity, and completeness for the specific role.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba9f56b7",
        "outputId": "f6cbb20b-e870-45f5-fe98-c666e184af9e"
      },
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Ensure the environment variable is set. This is a re-attempt.\n",
        "# In a real scenario, the user should ensure this is set correctly in the\n",
        "# 'Set up the environment' subtask with their actual key.\n",
        "try:\n",
        "    genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "    evaluation_model = genai.GenerativeModel('gemini-1.5-flash') # Or choose a different model\n",
        "    print(\"Google Generative AI configured successfully for evaluation.\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to configure Google Generative AI for evaluation: {e}\")\n",
        "    evaluation_model = None # Set model to None if configuration fails\n",
        "\n",
        "\n",
        "def evaluate_answer(question, user_answer, role, model):\n",
        "    \"\"\"\n",
        "    Evaluates a user's answer to a question based on correctness, clarity, and completeness for a given role.\n",
        "\n",
        "    Args:\n",
        "        question: The interview question asked.\n",
        "        user_answer: The user's response to the question.\n",
        "        role: The role for which the interview is being conducted (e.g., \"Mean Stack\").\n",
        "        model: The generative AI model to use for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the structured evaluation from the language model, or an error message.\n",
        "    \"\"\"\n",
        "    if model is None:\n",
        "        return \"Evaluation model not configured. Cannot evaluate answer.\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an experienced technical interviewer. Evaluate the following answer provided by a candidate for a {role} role.\n",
        "    Assess the answer based on three criteria:\n",
        "    1.  **Correctness (Technical Accuracy):** Is the technical information accurate? Are there any factual errors or misconceptions?\n",
        "    2.  **Clarity (Ease of Understanding):** Is the answer easy to understand? Is it well-structured and logically presented?\n",
        "    3.  **Completeness (Coverage of Relevant Aspects):** Does the answer fully address the question? Does it cover the key points and considerations relevant to the {role} role?\n",
        "\n",
        "    Provide a structured evaluation for the candidate's answer. Include a qualitative assessment or a score (e.g., on a scale of 1-5 or Poor/Fair/Good/Excellent) for each criterion. Also, provide brief feedback explaining your assessment for each criterion.\n",
        "\n",
        "    **Interview Question:**\n",
        "    {question}\n",
        "\n",
        "    **Candidate's Answer:**\n",
        "    {user_answer}\n",
        "\n",
        "    **Evaluation:**\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred during evaluation: {e}\"\n",
        "\n",
        "# Example usage (assuming evaluation_model is configured)\n",
        "# Note: This is for demonstration and will be used when integrating with the interview flow.\n",
        "# evaluated_feedback = evaluate_answer(\n",
        "#     \"Explain the concept of Virtual DOM in React.\",\n",
        "#     \"The Virtual DOM is a lightweight copy of the actual DOM. When the state of a component changes, React updates the Virtual DOM, compares it to the previous version, and then updates only the necessary parts of the real DOM. This makes updates faster.\",\n",
        "#     \"MERN Stack\",\n",
        "#     evaluation_model\n",
        "# )\n",
        "# print(\"\\nEvaluation:\")\n",
        "# print(evaluated_feedback)\n",
        "\n",
        "# Another example\n",
        "# evaluated_feedback_ai = evaluate_answer(\n",
        "#     \"What is the difference between supervised and unsupervised learning?\",\n",
        "#     \"Supervised learning uses labeled data to train models, while unsupervised learning uses unlabeled data to find patterns.\",\n",
        "#     \"AI\",\n",
        "#     evaluation_model\n",
        "# )\n",
        "# print(\"\\nEvaluation:\")\n",
        "# print(evaluated_feedback_ai)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Generative AI configured successfully for evaluation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e699c8d3"
      },
      "source": [
        "## Provide feedback\n",
        "\n",
        "### Subtask:\n",
        "Generate constructive and friendly feedback based on the evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccf0187c"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `generate_feedback` function as instructed, using the evaluation model to synthesize the structured evaluation into friendly and constructive feedback.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "1a2b58c4",
        "outputId": "7b742468-c756-4666-9bc7-f4f3ec4bec7c"
      },
      "source": [
        "def generate_feedback(evaluation, user_answer, question, model):\n",
        "    \"\"\"\n",
        "    Generates friendly and constructive feedback based on the evaluation.\n",
        "\n",
        "    Args:\n",
        "        evaluation: The structured evaluation string from the evaluate_answer function.\n",
        "        user_answer: The user's original answer to the question.\n",
        "        question: The question that was asked.\n",
        "        model: The generative AI model to use for generating feedback.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the friendly and constructive feedback.\n",
        "    \"\"\"\n",
        "    if model is None:\n",
        "        return \"Feedback model not configured. Cannot generate feedback.\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a friendly and encouraging AI interviewer providing feedback to a candidate.\n",
        "    Based on the following structured evaluation and the candidate's answer to the question,\n",
        "    synthesize the information into constructive and actionable feedback.\n",
        "    Highlight the strengths of the answer and suggest specific areas for improvement based on the correctness, clarity, and completeness criteria mentioned in the evaluation.\n",
        "    Maintain a supportive and encouraging tone.\n",
        "\n",
        "    **Interview Question:**\n",
        "    {question}\n",
        "\n",
        "    **Candidate's Answer:**\n",
        "    {user_answer}\n",
        "\n",
        "    **Structured Evaluation:**\n",
        "    {evaluation}\n",
        "\n",
        "    **Constructive Feedback:**\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred during feedback generation: {e}\"\n",
        "\n",
        "# Example usage (assuming evaluation_model is configured and evaluate_answer was successful)\n",
        "# We will use a sample evaluation string for demonstration.\n",
        "sample_question = \"Explain the concept of Virtual DOM in React.\"\n",
        "sample_user_answer = \"The Virtual DOM is a lightweight copy of the actual DOM. When the state of a component changes, React updates the Virtual DOM, compares it to the previous version, and then updates only the necessary parts of the real DOM. This makes updates faster.\"\n",
        "sample_evaluation = \"\"\"\n",
        "Evaluation:\n",
        "1.  **Correctness (Technical Accuracy):** Excellent (5/5). The technical explanation of the Virtual DOM and its role in React's reconciliation process is accurate.\n",
        "2.  **Clarity (Ease of Understanding):** Good (4/5). The explanation is generally easy to understand, though a brief mention of the reconciliation algorithm could enhance clarity further.\n",
        "3.  **Completeness (Coverage of Relevant Aspects):** Good (4/5). The answer covers the core concept and its benefit (speed). Mentioning the \"diffing\" algorithm explicitly would make it more complete.\n",
        "\"\"\"\n",
        "\n",
        "if 'evaluation_model' in locals() and evaluation_model is not None:\n",
        "    generated_feedback = generate_feedback(\n",
        "        sample_evaluation,\n",
        "        sample_user_answer,\n",
        "        sample_question,\n",
        "        evaluation_model\n",
        "    )\n",
        "    print(\"\\nGenerated Feedback:\")\n",
        "    print(generated_feedback)\n",
        "else:\n",
        "    print(\"\\nEvaluation model not available, skipping feedback generation example.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Feedback:\n",
            "An error occurred during feedback generation: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key not valid. Please pass a valid API key.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 3950.16ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c246a3e"
      },
      "source": [
        "## Iterate and improve\n",
        "\n",
        "### Subtask:\n",
        "Continuously refine the questions, evaluation criteria, and feedback mechanism based on user interaction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3624aa8"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement a mechanism to collect user feedback and store it in a structured format. This involves creating a function or class to handle feedback submission and a simple data structure (like a list of dictionaries) or file to store it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "a55f5331",
        "outputId": "77730876-c4bd-4f86-ea66-4748407788b1"
      },
      "source": [
        "import json\n",
        "\n",
        "# Using a list of dictionaries in memory for simplicity.\n",
        "# In a production system, this would be stored in a database or file.\n",
        "user_feedback_data = []\n",
        "\n",
        "def collect_feedback(interview_id, question, user_answer, generated_feedback, user_rating, user_comments):\n",
        "    \"\"\"\n",
        "    Collects user feedback on a specific interview question and the generated feedback.\n",
        "\n",
        "    Args:\n",
        "        interview_id: A unique identifier for the interview session.\n",
        "        question: The interview question.\n",
        "        user_answer: The user's answer to the question.\n",
        "        generated_feedback: The feedback provided by the AI.\n",
        "        user_rating: A numerical rating (e.g., 1-5) for the feedback.\n",
        "        user_comments: Text comments from the user.\n",
        "    \"\"\"\n",
        "    feedback_entry = {\n",
        "        \"interview_id\": interview_id,\n",
        "        \"question\": question,\n",
        "        \"user_answer\": user_answer,\n",
        "        \"generated_feedback\": generated_feedback,\n",
        "        \"user_rating\": user_rating,\n",
        "        \"user_comments\": user_comments,\n",
        "        \"timestamp\": pd.Timestamp.now().isoformat() # Using pandas for timestamp\n",
        "    }\n",
        "    user_feedback_data.append(feedback_entry)\n",
        "    print(\"Feedback collected successfully.\")\n",
        "\n",
        "# Example of how feedback would be collected after an interview question\n",
        "# assuming you have the necessary variables from the interview flow:\n",
        "# collect_feedback(\n",
        "#     interview_id=\"interview_123\",\n",
        "#     question=sample_question, # from previous steps\n",
        "#     user_answer=sample_user_answer, # from previous steps\n",
        "#     generated_feedback=generated_feedback, # from previous steps\n",
        "#     user_rating=4, # User provides a rating\n",
        "#     user_comments=\"The feedback was helpful, but could be more specific on area X.\"\n",
        "# )\n",
        "\n",
        "# For demonstration, let's add a couple of sample feedback entries\n",
        "if not user_feedback_data: # Add only if the list is empty\n",
        "    collect_feedback(\n",
        "        interview_id=\"interview_demo_1\",\n",
        "        question=\"Explain hoisting in JavaScript.\",\n",
        "        user_answer=\"Hoisting moves variable and function declarations to the top of their scope.\",\n",
        "        generated_feedback=\"Good explanation of hoisting. Mentioning the difference between var, let, and const would improve completeness.\",\n",
        "        user_rating=5,\n",
        "        user_comments=\"Clear and concise feedback.\"\n",
        "    )\n",
        "\n",
        "    collect_feedback(\n",
        "        interview_id=\"interview_demo_1\",\n",
        "        question=\"What are React Hooks?\",\n",
        "        user_answer=\"Hooks are functions that let you use state and other React features without writing a class.\",\n",
        "        generated_feedback=\"Correct definition. Elaborating on specific hooks like useState or useEffect would be beneficial.\",\n",
        "        user_rating=4,\n",
        "        user_comments=\"Helpful, but could provide examples.\"\n",
        "    )\n",
        "\n",
        "    collect_feedback(\n",
        "        interview_id=\"interview_demo_2\",\n",
        "        question=\"Describe gradient descent.\",\n",
        "        user_answer=\"It's an optimization algorithm to find the minimum of a function.\",\n",
        "        generated_feedback=\"Basic definition is correct. Explaining the steps (learning rate, iterations) and types (batch, stochastic) is crucial for a complete answer.\",\n",
        "        user_rating=3,\n",
        "        user_comments=\"Feedback was a bit too general.\"\n",
        "    )\n",
        "\n",
        "# Display collected feedback (for verification)\n",
        "print(\"\\nCollected Feedback Data:\")\n",
        "# Convert to DataFrame for better display if pandas is available\n",
        "try:\n",
        "    import pandas as pd\n",
        "    display(pd.DataFrame(user_feedback_data))\n",
        "except ImportError:\n",
        "    print(json.dumps(user_feedback_data, indent=2))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-3861676863.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# For demonstration, let's add a couple of sample feedback entries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muser_feedback_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Add only if the list is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     collect_feedback(\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0minterview_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"interview_demo_1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Explain hoisting in JavaScript.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-9-3861676863.py\u001b[0m in \u001b[0;36mcollect_feedback\u001b[0;34m(interview_id, question, user_answer, generated_feedback, user_rating, user_comments)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;34m\"user_rating\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_rating\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;34m\"user_comments\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_comments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Using pandas for timestamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     }\n\u001b[1;32m     28\u001b[0m     \u001b[0muser_feedback_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeedback_entry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d252c548"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `pd` (pandas) was used without being imported. The timestamp collection should be fixed, and the rest of the code for collecting and displaying feedback is correct. I will fix the timestamp collection to use the `datetime` module from the standard library instead of pandas, and re-execute the code block to collect and display sample feedback.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "eNc-CcVofqfa",
        "outputId": "4f8e6a5c-c9e4-45d1-e667-5f42d9b61844"
      },
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Using a list of dictionaries in memory for simplicity.\n",
        "# In a production system, this would be stored in a database or file.\n",
        "user_feedback_data = []\n",
        "\n",
        "def collect_feedback(interview_id, question, user_answer, generated_feedback, user_rating, user_comments):\n",
        "    \"\"\"\n",
        "    Collects user feedback on a specific interview question and the generated feedback.\n",
        "\n",
        "    Args:\n",
        "        interview_id: A unique identifier for the interview session.\n",
        "        question: The interview question.\n",
        "        user_answer: The user's answer to the question.\n",
        "        generated_feedback: The feedback provided by the AI.\n",
        "        user_rating: A numerical rating (e.g., 1-5) for the feedback.\n",
        "        user_comments: Text comments from the user.\n",
        "    \"\"\"\n",
        "    feedback_entry = {\n",
        "        \"interview_id\": interview_id,\n",
        "        \"question\": question,\n",
        "        \"user_answer\": user_answer,\n",
        "        \"generated_feedback\": generated_feedback,\n",
        "        \"user_rating\": user_rating,\n",
        "        \"user_comments\": user_comments,\n",
        "        \"timestamp\": datetime.now().isoformat() # Using datetime for timestamp\n",
        "    }\n",
        "    user_feedback_data.append(feedback_entry)\n",
        "    print(\"Feedback collected successfully.\")\n",
        "\n",
        "# Example of how feedback would be collected after an interview question\n",
        "# assuming you have the necessary variables from the interview flow:\n",
        "# collect_feedback(\n",
        "#     interview_id=\"interview_123\",\n",
        "#     question=sample_question, # from previous steps\n",
        "#     user_answer=sample_user_answer, # from previous steps\n",
        "#     generated_feedback=generated_feedback, # from previous steps\n",
        "#     user_rating=4, # User provides a rating\n",
        "#     user_comments=\"The feedback was helpful, but could be more specific on area X.\"\n",
        "# )\n",
        "\n",
        "# For demonstration, let's add a couple of sample feedback entries\n",
        "if not user_feedback_data: # Add only if the list is empty\n",
        "    collect_feedback(\n",
        "        interview_id=\"interview_demo_1\",\n",
        "        question=\"Explain hoisting in JavaScript.\",\n",
        "        user_answer=\"Hoisting moves variable and function declarations to the top of their scope.\",\n",
        "        generated_feedback=\"Good explanation of hoisting. Mentioning the difference between var, let, and const would improve completeness.\",\n",
        "        user_rating=5,\n",
        "        user_comments=\"Clear and concise feedback.\"\n",
        "    )\n",
        "\n",
        "    collect_feedback(\n",
        "        interview_id=\"interview_demo_1\",\n",
        "        question=\"What are React Hooks?\",\n",
        "        user_answer=\"Hooks are functions that let you use state and other React features without writing a class.\",\n",
        "        generated_feedback=\"Correct definition. Elaborating on specific hooks like useState or useEffect would be beneficial.\",\n",
        "        user_rating=4,\n",
        "        user_comments=\"Helpful, but could provide examples.\"\n",
        "    )\n",
        "\n",
        "    collect_feedback(\n",
        "        interview_id=\"interview_demo_2\",\n",
        "        question=\"Describe gradient descent.\",\n",
        "        user_answer=\"It's an optimization algorithm to find the minimum of a function.\",\n",
        "        generated_feedback=\"Basic definition is correct. Explaining the steps (learning rate, iterations) and types (batch, stochastic) is crucial for a complete answer.\",\n",
        "        user_rating=3,\n",
        "        user_comments=\"Feedback was a bit too general.\"\n",
        "    )\n",
        "\n",
        "# Display collected feedback (for verification)\n",
        "print(\"\\nCollected Feedback Data:\")\n",
        "# Convert to DataFrame for better display if pandas is available\n",
        "try:\n",
        "    import pandas as pd\n",
        "    display(pd.DataFrame(user_feedback_data))\n",
        "except ImportError:\n",
        "    print(json.dumps(user_feedback_data, indent=2))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feedback collected successfully.\n",
            "Feedback collected successfully.\n",
            "Feedback collected successfully.\n",
            "\n",
            "Collected Feedback Data:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       interview_id                         question  \\\n",
              "0  interview_demo_1  Explain hoisting in JavaScript.   \n",
              "1  interview_demo_1            What are React Hooks?   \n",
              "2  interview_demo_2       Describe gradient descent.   \n",
              "\n",
              "                                         user_answer  \\\n",
              "0  Hoisting moves variable and function declarati...   \n",
              "1  Hooks are functions that let you use state and...   \n",
              "2  It's an optimization algorithm to find the min...   \n",
              "\n",
              "                                  generated_feedback  user_rating  \\\n",
              "0  Good explanation of hoisting. Mentioning the d...            5   \n",
              "1  Correct definition. Elaborating on specific ho...            4   \n",
              "2  Basic definition is correct. Explaining the st...            3   \n",
              "\n",
              "                          user_comments                   timestamp  \n",
              "0           Clear and concise feedback.  2025-07-17T17:46:26.523890  \n",
              "1  Helpful, but could provide examples.  2025-07-17T17:46:26.523944  \n",
              "2       Feedback was a bit too general.  2025-07-17T17:46:26.523954  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cbdc4c25-fb5e-4fa4-adce-f6ed2a8627f2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>interview_id</th>\n",
              "      <th>question</th>\n",
              "      <th>user_answer</th>\n",
              "      <th>generated_feedback</th>\n",
              "      <th>user_rating</th>\n",
              "      <th>user_comments</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>interview_demo_1</td>\n",
              "      <td>Explain hoisting in JavaScript.</td>\n",
              "      <td>Hoisting moves variable and function declarati...</td>\n",
              "      <td>Good explanation of hoisting. Mentioning the d...</td>\n",
              "      <td>5</td>\n",
              "      <td>Clear and concise feedback.</td>\n",
              "      <td>2025-07-17T17:46:26.523890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>interview_demo_1</td>\n",
              "      <td>What are React Hooks?</td>\n",
              "      <td>Hooks are functions that let you use state and...</td>\n",
              "      <td>Correct definition. Elaborating on specific ho...</td>\n",
              "      <td>4</td>\n",
              "      <td>Helpful, but could provide examples.</td>\n",
              "      <td>2025-07-17T17:46:26.523944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>interview_demo_2</td>\n",
              "      <td>Describe gradient descent.</td>\n",
              "      <td>It's an optimization algorithm to find the min...</td>\n",
              "      <td>Basic definition is correct. Explaining the st...</td>\n",
              "      <td>3</td>\n",
              "      <td>Feedback was a bit too general.</td>\n",
              "      <td>2025-07-17T17:46:26.523954</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cbdc4c25-fb5e-4fa4-adce-f6ed2a8627f2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cbdc4c25-fb5e-4fa4-adce-f6ed2a8627f2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cbdc4c25-fb5e-4fa4-adce-f6ed2a8627f2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-bb6ba13d-1190-4c4e-a56a-b2c34126870d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bb6ba13d-1190-4c4e-a56a-b2c34126870d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-bb6ba13d-1190-4c4e-a56a-b2c34126870d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(json\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"interview_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"interview_demo_2\",\n          \"interview_demo_1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Explain hoisting in JavaScript.\",\n          \"What are React Hooks?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user_answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Hoisting moves variable and function declarations to the top of their scope.\",\n          \"Hooks are functions that let you use state and other React features without writing a class.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"generated_feedback\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Good explanation of hoisting. Mentioning the difference between var, let, and const would improve completeness.\",\n          \"Correct definition. Elaborating on specific hooks like useState or useEffect would be beneficial.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user_rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 3,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          5,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user_comments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Clear and concise feedback.\",\n          \"Helpful, but could provide examples.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2025-07-17T17:46:26.523890\",\n          \"2025-07-17T17:46:26.523944\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "951b6a65"
      },
      "source": [
        "**Reasoning**:\n",
        "Analyze the collected feedback to identify common themes and areas for improvement. This involves processing the `user_feedback_data` list to summarize ratings and comments, looking for recurring patterns in the feedback content.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b2d3159",
        "outputId": "eaa23020-b469-4bf1-8d9c-9bfc5fa0f383"
      },
      "source": [
        "# Analyze the collected feedback data\n",
        "\n",
        "def analyze_feedback(feedback_list):\n",
        "    \"\"\"\n",
        "    Analyzes the collected user feedback to identify themes and areas for improvement.\n",
        "\n",
        "    Args:\n",
        "        feedback_list: A list of feedback dictionaries.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary summarizing the analysis, including average rating,\n",
        "        common comments, and identified areas for improvement.\n",
        "    \"\"\"\n",
        "    if not feedback_list:\n",
        "        return {\"message\": \"No feedback data to analyze.\"}\n",
        "\n",
        "    total_ratings = 0\n",
        "    comments = []\n",
        "    # Simple approach to identify potential areas for improvement based on comments\n",
        "    improvement_areas = {}\n",
        "\n",
        "    for entry in feedback_list:\n",
        "        total_ratings += entry.get(\"user_rating\", 0)\n",
        "        comment = entry.get(\"user_comments\")\n",
        "        if comment:\n",
        "            comments.append(comment)\n",
        "            # Basic keyword analysis (can be enhanced)\n",
        "            comment_lower = comment.lower()\n",
        "            if \"clear\" in comment_lower or \"clarity\" in comment_lower:\n",
        "                 improvement_areas[\"clarity\"] = improvement_areas.get(\"clarity\", 0) + 1\n",
        "            if \"specific\" in comment_lower:\n",
        "                 improvement_areas[\"specificity\"] = improvement_areas.get(\"specificity\", 0) + 1\n",
        "            if \"examples\" in comment_lower:\n",
        "                 improvement_areas[\"examples\"] = improvement_areas.get(\"examples\", 0) + 1\n",
        "            if \"general\" in comment_lower:\n",
        "                 improvement_areas[\"generality\"] = improvement_areas.get(\"generality\", 0) + 1\n",
        "            if \"helpful\" in comment_lower:\n",
        "                 improvement_areas[\"helpfulness\"] = improvement_areas.get(\"helpfulness\", 0) + 1\n",
        "            # Add more keywords related to correctness, completeness, tone, etc.\n",
        "\n",
        "    average_rating = total_ratings / len(feedback_list) if feedback_list else 0\n",
        "\n",
        "    # For common comments, a more sophisticated approach (like topic modeling)\n",
        "    # would be needed for a large dataset. For now, we just list them.\n",
        "    print(\"\\nIndividual Comments:\")\n",
        "    for i, comment in enumerate(comments):\n",
        "        print(f\"- {comment}\")\n",
        "\n",
        "    print(\"\\nIdentified Themes/Areas (based on basic keyword analysis):\")\n",
        "    for area, count in improvement_areas.items():\n",
        "        print(f\"- {area.capitalize()}: {count} mentions\")\n",
        "\n",
        "\n",
        "    analysis_summary = {\n",
        "        \"average_rating\": average_rating,\n",
        "        \"total_feedback_entries\": len(feedback_list),\n",
        "        \"identified_improvement_areas\": improvement_areas\n",
        "        # In a real system, you might add common phrases, sentiment analysis, etc.\n",
        "    }\n",
        "\n",
        "    return analysis_summary\n",
        "\n",
        "# Perform analysis on the collected feedback data\n",
        "feedback_analysis_summary = analyze_feedback(user_feedback_data)\n",
        "\n",
        "print(\"\\nFeedback Analysis Summary:\")\n",
        "print(json.dumps(feedback_analysis_summary, indent=2))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Individual Comments:\n",
            "- Clear and concise feedback.\n",
            "- Helpful, but could provide examples.\n",
            "- Feedback was a bit too general.\n",
            "\n",
            "Identified Themes/Areas (based on basic keyword analysis):\n",
            "- Clarity: 1 mentions\n",
            "- Examples: 1 mentions\n",
            "- Helpfulness: 1 mentions\n",
            "- Generality: 1 mentions\n",
            "\n",
            "Feedback Analysis Summary:\n",
            "{\n",
            "  \"average_rating\": 4.0,\n",
            "  \"total_feedback_entries\": 3,\n",
            "  \"identified_improvement_areas\": {\n",
            "    \"clarity\": 1,\n",
            "    \"examples\": 1,\n",
            "    \"helpfulness\": 1,\n",
            "    \"generality\": 1\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b5a57f3"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The project successfully set up the environment by installing necessary libraries (`transformers`, `openai`, `google-generativeai`), although configuring API keys requires user intervention.\n",
        "*   Detailed personality traits, tone, and style for a friendly AI interviewer were clearly defined.\n",
        "*   The attempt to generate interview questions using Google's Generative AI model (`gemini-1.5-flash`) failed repeatedly due to an invalid API key configuration.\n",
        "*   A core function `conduct_interview` was successfully developed to simulate an interview flow, presenting questions and collecting user responses with an early exit option.\n",
        "*   A method `evaluate_answer` was successfully defined to evaluate user responses based on correctness, clarity, and completeness using a generative AI model, although its execution also failed due to the invalid API key.\n",
        "*   The `generate_feedback` function was correctly implemented to generate friendly and constructive feedback based on an evaluation, but it could not be demonstrated due to the same API key issue.\n",
        "*   A mechanism for collecting and storing user feedback (`collect_feedback`) was successfully implemented, allowing for the recording of user ratings and comments on the AI's performance and feedback.\n",
        "*   A basic feedback analysis function (`analyze_feedback`) was developed to process collected feedback, calculate average ratings, and identify potential areas for improvement through keyword analysis of comments.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Resolving the invalid API key issue is the critical next step to enable the core functionalities of question generation, answer evaluation, and feedback generation.\n",
        "*   Once the AI model can be successfully accessed, integrate the `conduct_interview`, `evaluate_answer`, and `generate_feedback` functions into a seamless mock interview flow.\n",
        "*   Expand the feedback analysis mechanism to include more sophisticated techniques (e.g., sentiment analysis, topic modeling) for larger datasets to gain deeper insights into areas for improvement in the AI's performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70bb8da3"
      },
      "source": [
        "# Task\n",
        "Create a friendly AI interviewer using Gemini API and RAG for Mean Stack, MERN Stack, and AI roles, capable of asking relevant questions, evaluating responses, and providing feedback to help users prepare for interviews. Include RAG and prompt engineering in the solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a54f8eb"
      },
      "source": [
        "## Define rag knowledge sources\n",
        "\n",
        "### Subtask:\n",
        "Identify and prepare relevant documents or data (e.g., technical concepts, common interview questions/answers for Mean Stack, MERN Stack, AI) to serve as the knowledge base for RAG.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53241ed3"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to create a variable to store the knowledge base for RAG and populate it with relevant information for the specified roles. I will use a dictionary where keys are roles and values are lists of strings containing relevant information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac69efa3",
        "outputId": "0ec29df8-25de-400c-ea39-040401d28ceb"
      },
      "source": [
        "# Create a dictionary to store the knowledge base for RAG\n",
        "rag_knowledge_base = {\n",
        "    \"Mean Stack\": [\n",
        "        \"MongoDB: NoSQL database, stores data in flexible, JSON-like documents.\",\n",
        "        \"Express.js: Backend web application framework for Node.js.\",\n",
        "        \"Angular: Frontend framework for building single-page applications.\",\n",
        "        \"Node.js: JavaScript runtime environment for server-side programming.\",\n",
        "        \"Common Mean Stack interview questions: Explain the role of each component in the MEAN stack. How do you handle routing in Express.js and Angular? Describe how data flows through a MEAN application. What are some advantages and disadvantages of using the MEAN stack?\",\n",
        "        \"Schema design in MongoDB: Embedding vs. referencing.\",\n",
        "        \"RESTful API design principles.\",\n",
        "        \"Authentication and authorization in web applications.\",\n",
        "        \"Asynchronous programming in Node.js (callbacks, promises, async/await).\",\n",
        "        \"Dependency injection in Angular.\",\n",
        "    ],\n",
        "    \"MERN Stack\": [\n",
        "        \"MongoDB: NoSQL database, stores data in flexible, JSON-like documents.\",\n",
        "        \"Express.js: Backend web application framework for Node.js.\",\n",
        "        \"React: Frontend JavaScript library for building user interfaces.\",\n",
        "        \"Node.js: JavaScript runtime environment for server-side programming.\",\n",
        "        \"Common MERN Stack interview questions: How does React's Virtual DOM work? Explain state management in React (useState, useReducer, Context API, Redux). How do you connect React frontend to an Express.js backend? Describe server-side rendering in React. What are the benefits of using the MERN stack?\",\n",
        "        \"React Hooks (useState, useEffect, useContext, etc.).\",\n",
        "        \"Component lifecycle in React.\",\n",
        "        \"Handling forms in React.\",\n",
        "        \"Middleware in Express.js.\",\n",
        "        \"Error handling in MERN applications.\",\n",
        "    ],\n",
        "    \"AI\": [\n",
        "        \"Machine Learning: Algorithms that allow computers to learn from data without being explicitly programmed.\",\n",
        "        \"Deep Learning: A subset of machine learning that uses artificial neural networks with multiple layers.\",\n",
        "        \"Supervised Learning: Training a model on labeled data (e.g., classification, regression).\",\n",
        "        \"Unsupervised Learning: Finding patterns in unlabeled data (e.g., clustering, dimensionality reduction).\",\n",
        "        \"Reinforcement Learning: Training agents to make sequential decisions in an environment to maximize a reward.\",\n",
        "        \"Common AI interview questions: Explain the difference between supervised and unsupervised learning. What is overfitting and how can you prevent it? Describe the concept of a convolutional neural network (CNN). How do you evaluate the performance of a machine learning model? Explain the bias-variance tradeoff.\",\n",
        "        \"Neural networks: Architecture (layers, neurons, activation functions).\",\n",
        "        \"Evaluation metrics (accuracy, precision, recall, F1-score, RMSE).\",\n",
        "        \"Data preprocessing techniques (cleaning, scaling, handling missing values).\",\n",
        "        \"Regularization techniques (L1, L2, dropout).\",\n",
        "        \"Natural Language Processing (NLP) basics.\",\n",
        "        \"Computer Vision basics.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Display a small part of the knowledge base to verify\n",
        "print(\"Mean Stack Knowledge Base (first 3 entries):\")\n",
        "for entry in rag_knowledge_base[\"Mean Stack\"][:3]:\n",
        "    print(f\"- {entry}\")\n",
        "\n",
        "print(\"\\nMERN Stack Knowledge Base (first 3 entries):\")\n",
        "for entry in rag_knowledge_base[\"MERN Stack\"][:3]:\n",
        "    print(f\"- {entry}\")\n",
        "\n",
        "print(\"\\nAI Knowledge Base (first 3 entries):\")\n",
        "for entry in rag_knowledge_base[\"AI\"][:3]:\n",
        "    print(f\"- {entry}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Stack Knowledge Base (first 3 entries):\n",
            "- MongoDB: NoSQL database, stores data in flexible, JSON-like documents.\n",
            "- Express.js: Backend web application framework for Node.js.\n",
            "- Angular: Frontend framework for building single-page applications.\n",
            "\n",
            "MERN Stack Knowledge Base (first 3 entries):\n",
            "- MongoDB: NoSQL database, stores data in flexible, JSON-like documents.\n",
            "- Express.js: Backend web application framework for Node.js.\n",
            "- React: Frontend JavaScript library for building user interfaces.\n",
            "\n",
            "AI Knowledge Base (first 3 entries):\n",
            "- Machine Learning: Algorithms that allow computers to learn from data without being explicitly programmed.\n",
            "- Deep Learning: A subset of machine learning that uses artificial neural networks with multiple layers.\n",
            "- Supervised Learning: Training a model on labeled data (e.g., classification, regression).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cced818"
      },
      "source": [
        "## Implement rag retrieval\n",
        "\n",
        "### Subtask:\n",
        "Develop a mechanism to retrieve relevant information from the knowledge sources based on the current context (e.g., the role, the question being asked, the user's answer).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6573678"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `retrieve_rag_context` function to retrieve relevant information from the knowledge base based on the role and context.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2ddfb45",
        "outputId": "3d3f5021-2d54-4b6e-cb79-c485e9168ec8"
      },
      "source": [
        "def retrieve_rag_context(role, context):\n",
        "    \"\"\"\n",
        "    Retrieves relevant information from the knowledge base based on the role and context.\n",
        "\n",
        "    Args:\n",
        "        role: The interview role (e.g., \"Mean Stack\", \"MERN Stack\", \"AI\").\n",
        "        context: The current context (e.g., a question being asked, a user's answer).\n",
        "\n",
        "    Returns:\n",
        "        A list of relevant knowledge base entries.\n",
        "    \"\"\"\n",
        "    relevant_entries = []\n",
        "    if role in rag_knowledge_base:\n",
        "        knowledge_entries = rag_knowledge_base[role]\n",
        "        # Simple keyword matching (can be enhanced with embeddings for better relevance)\n",
        "        context_lower = context.lower()\n",
        "        for entry in knowledge_entries:\n",
        "            if any(keyword in entry.lower() for keyword in context_lower.split()):\n",
        "                relevant_entries.append(entry)\n",
        "            # Also check if the context is a subset of the entry (for longer entries matching shorter contexts)\n",
        "            elif context_lower in entry.lower():\n",
        "                 relevant_entries.append(entry)\n",
        "\n",
        "\n",
        "    # Remove duplicates and return\n",
        "    return list(set(relevant_entries))\n",
        "\n",
        "# Example usage (assuming rag_knowledge_base is defined)\n",
        "# Retrieve context for a Mean Stack question about Node.js\n",
        "context_mean = \"Explain asynchronous programming in Node.js.\"\n",
        "retrieved_mean_context = retrieve_rag_context(\"Mean Stack\", context_mean)\n",
        "print(f\"Retrieved context for '{context_mean}':\")\n",
        "for entry in retrieved_mean_context:\n",
        "    print(f\"- {entry}\")\n",
        "\n",
        "# Retrieve context for a MERN Stack question about React Hooks\n",
        "context_mern = \"What are React Hooks?\"\n",
        "retrieved_mern_context = retrieve_rag_context(\"MERN Stack\", context_mern)\n",
        "print(f\"\\nRetrieved context for '{context_mern}':\")\n",
        "for entry in retrieved_mern_context:\n",
        "    print(f\"- {entry}\")\n",
        "\n",
        "# Retrieve context for an AI question about supervised learning\n",
        "context_ai = \"Explain supervised learning.\"\n",
        "retrieved_ai_context = retrieve_rag_context(\"AI\", context_ai)\n",
        "print(f\"\\nRetrieved context for '{context_ai}':\")\n",
        "for entry in retrieved_ai_context:\n",
        "    print(f\"- {entry}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved context for 'Explain asynchronous programming in Node.js.':\n",
            "- Node.js: JavaScript runtime environment for server-side programming.\n",
            "- Schema design in MongoDB: Embedding vs. referencing.\n",
            "- RESTful API design principles.\n",
            "- Asynchronous programming in Node.js (callbacks, promises, async/await).\n",
            "- Common Mean Stack interview questions: Explain the role of each component in the MEAN stack. How do you handle routing in Express.js and Angular? Describe how data flows through a MEAN application. What are some advantages and disadvantages of using the MEAN stack?\n",
            "- Dependency injection in Angular.\n",
            "- Angular: Frontend framework for building single-page applications.\n",
            "- Express.js: Backend web application framework for Node.js.\n",
            "- MongoDB: NoSQL database, stores data in flexible, JSON-like documents.\n",
            "- Authentication and authorization in web applications.\n",
            "\n",
            "Retrieved context for 'What are React Hooks?':\n",
            "- Handling forms in React.\n",
            "- Component lifecycle in React.\n",
            "- Common MERN Stack interview questions: How does React's Virtual DOM work? Explain state management in React (useState, useReducer, Context API, Redux). How do you connect React frontend to an Express.js backend? Describe server-side rendering in React. What are the benefits of using the MERN stack?\n",
            "- Middleware in Express.js.\n",
            "- React: Frontend JavaScript library for building user interfaces.\n",
            "- React Hooks (useState, useEffect, useContext, etc.).\n",
            "\n",
            "Retrieved context for 'Explain supervised learning.':\n",
            "- Common AI interview questions: Explain the difference between supervised and unsupervised learning. What is overfitting and how can you prevent it? Describe the concept of a convolutional neural network (CNN). How do you evaluate the performance of a machine learning model? Explain the bias-variance tradeoff.\n",
            "- Unsupervised Learning: Finding patterns in unlabeled data (e.g., clustering, dimensionality reduction).\n",
            "- Supervised Learning: Training a model on labeled data (e.g., classification, regression).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a4510e4"
      },
      "source": [
        "## Generate interview questions (with rag and prompt engineering)\n",
        "\n",
        "### Subtask:\n",
        "Generate relevant interview questions for Mean Stack, MERN Stack, and AI roles, using prompt engineering and incorporating retrieved information from the knowledge base to ensure questions are relevant and in-depth.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cd76313"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `generate_interview_questions_rag` function as described in the instructions, incorporating RAG context into the prompt and handling potential API errors during question generation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "1d787305",
        "outputId": "94f87661-0532-4ba2-a440-a4ed2c5397b9"
      },
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Ensure the generative AI model is configured (assuming API key is set in environment)\n",
        "try:\n",
        "    # Explicitly get the API key from the environment variable here\n",
        "    api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "    if not api_key or api_key == \"YOUR_GOOGLE_API_KEY\":\n",
        "        raise ValueError(\"GOOGLE_API_KEY environment variable not set or is a placeholder.\")\n",
        "\n",
        "    genai.configure(api_key=api_key)\n",
        "    # Using gemini-1.5-flash as it's generally faster and more cost-effective\n",
        "    question_generation_model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "    print(\"Google Generative AI configured successfully for question generation.\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to configure Google Generative AI for question generation: {e}\")\n",
        "    question_generation_model = None # Set model to None if configuration fails\n",
        "\n",
        "\n",
        "def generate_interview_questions_rag(role, model):\n",
        "    \"\"\"\n",
        "    Generates a list of interview questions for a given role using a language model\n",
        "    and incorporating retrieved RAG context.\n",
        "\n",
        "    Args:\n",
        "        role: The role for the interview (e.g., \"Mean Stack\", \"MERN Stack\", \"AI\").\n",
        "        model: The generative AI model to use for question generation.\n",
        "\n",
        "    Returns:\n",
        "        A list of relevant interview questions, or an empty list if generation fails.\n",
        "    \"\"\"\n",
        "    if model is None:\n",
        "        print(f\"Question generation model not configured, cannot generate questions for {role}.\")\n",
        "        return []\n",
        "\n",
        "    # Retrieve relevant context from the knowledge base for the given role\n",
        "    # We'll use a broad context for question generation initially, maybe the role itself\n",
        "    # For more targeted questions, we could use specific sub-topics as context\n",
        "    retrieved_context = retrieve_rag_context(role, role) # Using the role name as context to get general info\n",
        "\n",
        "    context_string = \"\\n\".join(retrieved_context)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a friendly and knowledgeable AI interviewer preparing a candidate for an MNC interview for a {role} role.\n",
        "    Use the following information about the {role} to generate a list of relevant, challenging, but fair interview questions.\n",
        "    Include a mix of theoretical and practical questions, covering core concepts, technologies, problem-solving, and behavioral aspects specific to this role.\n",
        "    Structure the output as a numbered list of questions.\n",
        "    Provide at least 10 questions for the {role} role.\n",
        "\n",
        "    Relevant Information about {role}:\n",
        "    {context_string}\n",
        "\n",
        "    Interview Questions for {role}:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        # Assuming the response is text and can be split into a list of questions\n",
        "        questions = response.text.strip().split('\\n')\n",
        "        # Filter out any empty lines or non-question text\n",
        "        # Keep lines that start with a number followed by a period or parenthesis, and contain a question mark\n",
        "        questions = [q.strip() for q in questions if q.strip() and (q.strip().startswith(tuple(str(i) + '.' for i in range(100))) or q.strip().startswith(tuple(str(i) + ')' for i in range(100)))) and '?' in q]\n",
        "        # If fewer than 10 questions are generated, try a slightly less strict filter or regenerate.\n",
        "        # For this task, we'll stick to the filtered list.\n",
        "\n",
        "        return questions\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while generating questions for {role}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Generate questions for each role if the model was configured successfully\n",
        "mean_stack_questions_rag = []\n",
        "mern_stack_questions_rag = []\n",
        "ai_questions_rag = []\n",
        "\n",
        "if question_generation_model:\n",
        "    mean_stack_questions_rag = generate_interview_questions_rag(\"Mean Stack\", question_generation_model)\n",
        "    mern_stack_questions_rag = generate_interview_questions_rag(\"MERN Stack\", question_generation_model)\n",
        "    ai_questions_rag = generate_interview_questions_rag(\"AI\", question_generation_model)\n",
        "else:\n",
        "    print(\"Skipping question generation as the model was not configured.\")\n",
        "\n",
        "# Display the generated questions\n",
        "print(\"\\nMean Stack Interview Questions (RAG):\")\n",
        "if mean_stack_questions_rag:\n",
        "    for q in mean_stack_questions_rag:\n",
        "        print(q)\n",
        "else:\n",
        "    print(\"Could not generate Mean Stack questions.\")\n",
        "\n",
        "\n",
        "print(\"\\nMERN Stack Interview Questions (RAG):\")\n",
        "if mern_stack_questions_rag:\n",
        "    for q in mern_stack_questions_rag:\n",
        "        print(q)\n",
        "else:\n",
        "    print(\"Could not generate MERN Stack questions.\")\n",
        "\n",
        "\n",
        "print(\"\\nAI Interview Questions (RAG):\")\n",
        "if ai_questions_rag:\n",
        "    for q in ai_questions_rag:\n",
        "        print(q)\n",
        "else:\n",
        "    print(\"Could not generate AI questions.\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Generative AI configured successfully for question generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 37005.35ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2162.02ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while generating questions for Mean Stack: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2564.61ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while generating questions for MERN Stack: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "An error occurred while generating questions for AI: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "\n",
            "Mean Stack Interview Questions (RAG):\n",
            "Could not generate Mean Stack questions.\n",
            "\n",
            "MERN Stack Interview Questions (RAG):\n",
            "Could not generate MERN Stack questions.\n",
            "\n",
            "AI Interview Questions (RAG):\n",
            "Could not generate AI questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2062.27ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27871371"
      },
      "source": [
        "## Generate interview questions (with rag and prompt engineering) retry 1\n",
        "\n",
        "### Subtask:\n",
        "Generate relevant interview questions for Mean Stack, MERN Stack, and AI roles, using prompt engineering and incorporating retrieved information from the knowledge base to ensure questions are relevant and in-depth.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57e83db4"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to generate questions failed due to an API quota issue. Acknowledging this and the need for user action to resolve it, I will re-execute the code cell for question generation. The core logic of the function remains the same as it was designed to use RAG and prompt engineering, which is correct. This retry is to see if the quota issue has been resolved externally. I will include a print statement to indicate this retry and handle potential failure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "5abbcc04",
        "outputId": "1308c45f-58ad-4973-e7d6-5ed5faf4585a"
      },
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "print(\"Retrying question generation with RAG and prompt engineering.\")\n",
        "\n",
        "# Ensure the generative AI model is configured (assuming API key is set in environment)\n",
        "# Re-checking configuration in case environment changed\n",
        "try:\n",
        "    api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "    if not api_key or api_key == \"YOUR_GOOGLE_API_KEY\":\n",
        "        # This check was done before, but important to reiterate that a valid key is needed\n",
        "        raise ValueError(\"GOOGLE_API_KEY environment variable not set or is a placeholder.\")\n",
        "\n",
        "    # Configure only if not already configured successfully in a previous attempt\n",
        "    # This prevents re-configuring with a placeholder if the user hasn't updated it\n",
        "    if 'question_generation_model' not in locals() or question_generation_model is None:\n",
        "        genai.configure(api_key=api_key)\n",
        "        question_generation_model = genai.GenerativeModel('gemini-1.5-flash') # Or choose a different model\n",
        "        print(\"Google Generative AI configured successfully for question generation.\")\n",
        "    else:\n",
        "        print(\"Google Generative AI was already configured for question generation.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to configure Google Generative AI for question generation: {e}\")\n",
        "    question_generation_model = None # Set model to None if configuration fails\n",
        "\n",
        "\n",
        "# The generate_interview_questions_rag function remains the same as defined previously,\n",
        "# it will use the rag_knowledge_base and the configured model.\n",
        "# Assuming the previous cell where generate_interview_questions_rag was defined was executed successfully.\n",
        "\n",
        "# Generate questions for each role if the model was configured successfully\n",
        "mean_stack_questions_rag = []\n",
        "mern_stack_questions_rag = []\n",
        "ai_questions_rag = []\n",
        "\n",
        "if question_generation_model:\n",
        "    mean_stack_questions_rag = generate_interview_questions_rag(\"Mean Stack\", question_generation_model)\n",
        "    mern_stack_questions_rag = generate_interview_questions_rag(\"MERN Stack\", question_generation_model)\n",
        "    ai_questions_rag = generate_interview_questions_rag(\"AI\", question_generation_model)\n",
        "else:\n",
        "    print(\"Skipping question generation as the model was not configured.\")\n",
        "\n",
        "# Display the generated questions\n",
        "print(\"\\nMean Stack Interview Questions (RAG):\")\n",
        "if mean_stack_questions_rag:\n",
        "    for i, q in enumerate(mean_stack_questions_rag):\n",
        "        print(f\"{i+1}. {q}\")\n",
        "else:\n",
        "    print(\"Could not generate Mean Stack questions. Check API quota or key.\")\n",
        "\n",
        "\n",
        "print(\"\\nMERN Stack Interview Questions (RAG):\")\n",
        "if mern_stack_questions_rag:\n",
        "    for i, q in enumerate(mern_stack_questions_rag):\n",
        "        print(f\"{i+1}. {q}\")\n",
        "else:\n",
        "    print(\"Could not generate MERN Stack questions. Check API quota or key.\")\n",
        "\n",
        "\n",
        "print(\"\\nAI Interview Questions (RAG):\")\n",
        "if ai_questions_rag:\n",
        "    for i, q in enumerate(ai_questions_rag):\n",
        "        print(f\"{i+1}. {q}\")\n",
        "else:\n",
        "    print(\"Could not generate AI questions. Check API quota or key.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrying question generation with RAG and prompt engineering.\n",
            "Google Generative AI was already configured for question generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1609.86ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while generating questions for Mean Stack: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2013.42ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while generating questions for MERN Stack: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "An error occurred while generating questions for AI: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "\n",
            "Mean Stack Interview Questions (RAG):\n",
            "Could not generate Mean Stack questions. Check API quota or key.\n",
            "\n",
            "MERN Stack Interview Questions (RAG):\n",
            "Could not generate MERN Stack questions. Check API quota or key.\n",
            "\n",
            "AI Interview Questions (RAG):\n",
            "Could not generate AI questions. Check API quota or key.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1936.20ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd57d2c0"
      },
      "source": [
        "## Simulate an interview\n",
        "\n",
        "### Subtask:\n",
        "Create a flow to simulate an interview where the AI asks questions and the user responds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8a0fd7c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to generate questions failed due to an API quota issue. While I cannot fix the API quota, I can still define and execute the `conduct_interview` function using placeholder questions as instructed to demonstrate the interview flow simulation. The function is already defined in the provided notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb68efca",
        "outputId": "a0fe305e-dfc8-4475-b253-0eb916ce7644"
      },
      "source": [
        "# Assuming the conduct_interview function is defined in a previous cell\n",
        "# We will call the function with placeholder questions since real questions\n",
        "# could not be generated due to API errors.\n",
        "\n",
        "# Define placeholder questions as the real question generation failed\n",
        "placeholder_questions_mean = [\n",
        "    \"1. Can you explain the architecture of the MEAN stack?\",\n",
        "    \"2. How do you handle routing in an Express.js application?\",\n",
        "    \"3. Describe the role of Angular in a MEAN application.\",\n",
        "    \"4. How would you implement user authentication in a MEAN stack application?\",\n",
        "    \"5. What are some common challenges when working with MongoDB and how do you address them?\",\n",
        "]\n",
        "\n",
        "placeholder_questions_mern = [\n",
        "    \"1. Explain the concept of the Virtual DOM in React.\",\n",
        "    \"2. How do you manage state in a MERN application?\",\n",
        "    \"3. Describe the process of connecting a React frontend to an Express.js backend.\",\n",
        "    \"4. What are the advantages of using React Hooks?\",\n",
        "    \"5. How do you handle errors in a MERN stack application?\",\n",
        "]\n",
        "\n",
        "placeholder_questions_ai = [\n",
        "    \"1. What is the difference between supervised and unsupervised learning?\",\n",
        "    \"2. Explain the concept of overfitting and how to prevent it.\",\n",
        "    \"3. Describe the architecture of a simple neural network.\",\n",
        "    \"4. How do you evaluate the performance of a classification model?\",\n",
        "    \"5. What is gradient descent and how does it work?\",\n",
        "]\n",
        "\n",
        "\n",
        "# Conduct mock interviews using the placeholder questions\n",
        "print(\"--- Starting Mock Interviews with Placeholder Questions ---\")\n",
        "\n",
        "print(\"\\n--- Mean Stack Mock Interview ---\")\n",
        "mean_stack_responses = conduct_interview(\"Mean Stack\", placeholder_questions_mean)\n",
        "\n",
        "print(\"\\n--- MERN Stack Mock Interview ---\")\n",
        "mern_stack_responses = conduct_interview(\"MERN Stack\", placeholder_questions_mern)\n",
        "\n",
        "print(\"\\n--- AI Mock Interview ---\")\n",
        "ai_responses = conduct_interview(\"AI\", placeholder_questions_ai)\n",
        "\n",
        "print(\"\\n--- Mock Interviews Completed ---\")\n",
        "\n",
        "# Display the collected responses (optional)\n",
        "print(\"\\nMean Stack Interview Responses:\")\n",
        "if mean_stack_responses:\n",
        "    for response in mean_stack_responses:\n",
        "        print(f\"Q: {response['question']}\")\n",
        "        print(f\"A: {response['answer']}\")\n",
        "else:\n",
        "    print(\"No Mean Stack responses collected.\")\n",
        "\n",
        "print(\"\\nMERN Stack Interview Responses:\")\n",
        "if mern_stack_responses:\n",
        "    for response in mern_stack_responses:\n",
        "        print(f\"Q: {response['question']}\")\n",
        "        print(f\"A: {response['answer']}\")\n",
        "else:\n",
        "    print(\"No MERN Stack responses collected.\")\n",
        "\n",
        "print(\"\\nAI Interview Responses:\")\n",
        "if ai_responses:\n",
        "    for response in ai_responses:\n",
        "        print(f\"Q: {response['question']}\")\n",
        "        print(f\"A: {response['answer']}\")\n",
        "else:\n",
        "    print(\"No AI responses collected.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Mock Interviews with Placeholder Questions ---\n",
            "\n",
            "--- Mean Stack Mock Interview ---\n",
            "Starting mock interview for Mean Stack role.\n",
            "Type 'quit' at any time to end the interview.\n",
            "--------------------\n",
            "\n",
            "Question 1: 1. Can you explain the architecture of the MEAN stack?\n",
            "Your answer: work with angualr ,node and mongodb\n",
            "\n",
            "Question 2: 2. How do you handle routing in an Express.js application?\n",
            "Your answer: by using app.route\n",
            "\n",
            "Question 3: 3. Describe the role of Angular in a MEAN application.\n",
            "Your answer: as a frontend framework\n",
            "\n",
            "Question 4: 4. How would you implement user authentication in a MEAN stack application?\n",
            "Your answer: by using token of passport js very famous \n",
            "\n",
            "Question 5: 5. What are some common challenges when working with MongoDB and how do you address them?\n",
            "Your answer: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb5e41c7"
      },
      "source": [
        "## Evaluate responses (with rag and prompt engineering)\n",
        "\n",
        "### Subtask:\n",
        "Develop a method to evaluate the user's responses based on correctness, clarity, and completeness, using prompt engineering and leveraging retrieved information from the knowledge base to assess technical accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e3af3b9"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `evaluate_answer_rag` function as described in the instructions, incorporating RAG context retrieval and prompt engineering to evaluate the user's answer based on correctness, clarity, and completeness for the specific role. This function will utilize the previously defined `rag_knowledge_base` and `retrieve_rag_context` function, and the configured generative AI model. Then, demonstrate the function's usage with a sample question and answer if the evaluation model is configured.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85aac3d2",
        "outputId": "90069d0a-3920-4a0c-f578-ade43b77cb6d"
      },
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Ensure the generative AI model for evaluation is configured\n",
        "# Assuming this was attempted in previous steps, we'll check if the model variable exists and is not None.\n",
        "# If not configured, we'll attempt configuration again.\n",
        "try:\n",
        "    if 'evaluation_model' not in locals() or evaluation_model is None:\n",
        "        api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "        if not api_key or api_key == \"YOUR_GOOGLE_API_KEY\":\n",
        "            raise ValueError(\"GOOGLE_API_KEY environment variable not set or is a placeholder.\")\n",
        "        genai.configure(api_key=api_key)\n",
        "        evaluation_model = genai.GenerativeModel('gemini-1.5-flash') # Or choose a different model\n",
        "        print(\"Google Generative AI configured successfully for evaluation.\")\n",
        "    else:\n",
        "        print(\"Google Generative AI was already configured for evaluation.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to configure Google Generative AI for evaluation: {e}\")\n",
        "    evaluation_model = None # Set model to None if configuration fails\n",
        "\n",
        "\n",
        "def evaluate_answer_rag(question, user_answer, role, model):\n",
        "    \"\"\"\n",
        "    Evaluates a user's answer to a question based on correctness, clarity, and completeness\n",
        "    for a given role, leveraging retrieved RAG context.\n",
        "\n",
        "    Args:\n",
        "        question: The interview question asked.\n",
        "        user_answer: The user's response to the question.\n",
        "        role: The role for which the interview is being conducted (e.g., \"Mean Stack\").\n",
        "        model: The generative AI model to use for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the structured evaluation from the language model, or an error message.\n",
        "    \"\"\"\n",
        "    if model is None:\n",
        "        return \"Evaluation model not configured. Cannot evaluate answer.\"\n",
        "\n",
        "    # 1. Retrieve relevant information from the knowledge base using RAG\n",
        "    # Use the question and potentially keywords from the answer as context for retrieval\n",
        "    retrieval_context = f\"{role} {question} {user_answer}\"\n",
        "    retrieved_rag_context = retrieve_rag_context(role, retrieval_context)\n",
        "    context_string = \"\\n\".join(retrieved_rag_context)\n",
        "\n",
        "    # 2. Construct a detailed prompt for the generative AI model with RAG context\n",
        "    prompt = f\"\"\"\n",
        "    You are an experienced technical interviewer evaluating a candidate's response for a {role} role.\n",
        "    Use your expertise and the following provided knowledge base information to evaluate the candidate's answer.\n",
        "\n",
        "    **Knowledge Base Information:**\n",
        "    {context_string if context_string else \"No specific knowledge base information found for this question and role.\"}\n",
        "\n",
        "    Evaluate the candidate's answer to the interview question based on three criteria:\n",
        "    1.  **Correctness (Technical Accuracy):** Is the technical information accurate? Are there any factual errors or misconceptions? Use the knowledge base information to verify technical details.\n",
        "    2.  **Clarity (Ease of Understanding):** Is the answer easy to understand? Is it well-structured and logically presented?\n",
        "    3.  **Completeness (Coverage of Relevant Aspects):** Does the answer fully address the question? Does it cover the key points and considerations relevant to the {role} role, drawing upon the knowledge base information where applicable?\n",
        "\n",
        "    Provide a structured evaluation for the candidate's answer. Include a qualitative assessment (e.g., Poor, Fair, Good, Excellent) or a score (e.g., on a scale of 1-5) for each criterion. Also, provide brief feedback explaining your assessment for each criterion.\n",
        "\n",
        "    **Interview Question:**\n",
        "    {question}\n",
        "\n",
        "    **Candidate's Answer:**\n",
        "    {user_answer}\n",
        "\n",
        "    **Evaluation:**\n",
        "    \"\"\"\n",
        "\n",
        "    # 3. Call the generative AI model for evaluation\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        # 9. Implement error handling\n",
        "        return f\"An error occurred during evaluation: {e}\"\n",
        "\n",
        "# 11. Demonstrate the evaluate_answer_rag function if the model is configured\n",
        "print(\"\\n--- Demonstrating evaluate_answer_rag ---\")\n",
        "\n",
        "if 'evaluation_model' in locals() and evaluation_model is not None:\n",
        "    # Use a sample question and answer relevant to one of the roles\n",
        "    sample_question_rag = \"Explain the concept of overfitting and how to prevent it in machine learning.\"\n",
        "    sample_user_answer_rag = \"Overfitting is when a model performs well on training data but poorly on new data. You can prevent it by using more data, regularization, or cross-validation.\"\n",
        "    sample_role_rag = \"AI\"\n",
        "\n",
        "    print(f\"Evaluating sample answer for {sample_role_rag} role:\")\n",
        "    print(f\"Question: {sample_question_rag}\")\n",
        "    print(f\"Answer: {sample_user_answer_rag}\")\n",
        "\n",
        "    evaluated_feedback_rag = evaluate_answer_rag(\n",
        "        sample_question_rag,\n",
        "        sample_user_answer_rag,\n",
        "        sample_role_rag,\n",
        "        evaluation_model\n",
        "    )\n",
        "    print(\"\\nStructured Evaluation (RAG-informed):\")\n",
        "    print(evaluated_feedback_rag)\n",
        "else:\n",
        "    print(\"\\nEvaluation model not available, skipping evaluate_answer_rag demonstration.\")\n",
        "    print(\"Please ensure your GOOGLE_API_KEY is set correctly and you have sufficient quota.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to configure Google Generative AI for evaluation: GOOGLE_API_KEY environment variable not set or is a placeholder.\n",
            "\n",
            "--- Demonstrating evaluate_answer_rag ---\n",
            "\n",
            "Evaluation model not available, skipping evaluate_answer_rag demonstration.\n",
            "Please ensure your GOOGLE_API_KEY is set correctly and you have sufficient quota.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9419507"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to define and demonstrate the `evaluate_answer_rag` function failed because the `evaluation_model` was not successfully configured due to the `GOOGLE_API_KEY` environment variable not being set correctly. The instructions for this subtask require ensuring the environment variable is set. Although this was attempted in a previous subtask, the error persists. To proceed with demonstrating the `evaluate_answer_rag` function as required by the instructions, the most critical next step is to ensure the API key is correctly set in the environment and then re-attempt the configuration and demonstration. I will re-include the configuration step and the demonstration call within the same code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcdbc13c",
        "outputId": "0c8052c8-c2c9-4b5f-aadb-0a8ed7fa37f1"
      },
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "print(\"Retrying evaluation model configuration and evaluate_answer_rag demonstration.\")\n",
        "\n",
        "# Ensure the generative AI model for evaluation is configured\n",
        "try:\n",
        "    api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "    if not api_key or api_key == \"YOUR_GOOGLE_API_KEY\":\n",
        "        # This check is crucial. The user MUST set a valid key for this to work.\n",
        "        raise ValueError(\"GOOGLE_API_KEY environment variable not set or is a placeholder.\")\n",
        "\n",
        "    # Configure the model. This might fail if the key is invalid or quota is exceeded.\n",
        "    genai.configure(api_key=api_key)\n",
        "    evaluation_model = genai.GenerativeModel('gemini-1.5-flash') # Or choose a different model\n",
        "    print(\"Google Generative AI configured successfully for evaluation.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to configure Google Generative AI for evaluation: {e}\")\n",
        "    evaluation_model = None # Set model to None if configuration fails\n",
        "\n",
        "# The evaluate_answer_rag function is assumed to be defined in a previous cell.\n",
        "# If the evaluation model was configured successfully, demonstrate the function.\n",
        "print(\"\\n--- Demonstrating evaluate_answer_rag ---\")\n",
        "\n",
        "if 'evaluation_model' in locals() and evaluation_model is not None:\n",
        "    # Use a sample question and answer relevant to one of the roles\n",
        "    sample_question_rag = \"Explain the concept of overfitting and how to prevent it in machine learning.\"\n",
        "    sample_user_answer_rag = \"Overfitting is when a model performs well on training data but poorly on new data. You can prevent it by using more data, regularization, or cross-validation.\"\n",
        "    sample_role_rag = \"AI\"\n",
        "\n",
        "    print(f\"Evaluating sample answer for {sample_role_rag} role:\")\n",
        "    print(f\"Question: {sample_question_rag}\")\n",
        "    print(f\"Answer: {sample_user_answer_rag}\")\n",
        "\n",
        "    # Call the evaluate_answer_rag function\n",
        "    evaluated_feedback_rag = evaluate_answer_rag(\n",
        "        sample_question_rag,\n",
        "        sample_user_answer_rag,\n",
        "        sample_role_rag,\n",
        "        evaluation_model\n",
        "    )\n",
        "    print(\"\\nStructured Evaluation (RAG-informed):\")\n",
        "    print(evaluated_feedback_rag)\n",
        "else:\n",
        "    print(\"\\nEvaluation model not available, skipping evaluate_answer_rag demonstration.\")\n",
        "    print(\"Please ensure your GOOGLE_API_KEY is set correctly and you have sufficient quota.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrying evaluation model configuration and evaluate_answer_rag demonstration.\n",
            "Failed to configure Google Generative AI for evaluation: GOOGLE_API_KEY environment variable not set or is a placeholder.\n",
            "\n",
            "--- Demonstrating evaluate_answer_rag ---\n",
            "\n",
            "Evaluation model not available, skipping evaluate_answer_rag demonstration.\n",
            "Please ensure your GOOGLE_API_KEY is set correctly and you have sufficient quota.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb5f46ad"
      },
      "source": [
        "## Provide feedback (with rag and prompt engineering)\n",
        "\n",
        "### Subtask:\n",
        "Generate constructive and friendly feedback based on the evaluation, using prompt engineering and incorporating insights from the RAG-based evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ae179c9"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `generate_feedback_rag` function as instructed, using the evaluation model to synthesize the RAG-informed structured evaluation into friendly and constructive feedback, and then demonstrate the function with sample data if the evaluation model is configured.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65a0875a",
        "outputId": "bb95cf9b-f085-4507-fa84-2f43764b9077"
      },
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Ensure the generative AI model for feedback generation is configured\n",
        "# Assuming this was attempted in previous steps, we'll check if the model variable exists and is not None.\n",
        "# If not configured, we'll attempt configuration again.\n",
        "try:\n",
        "    # Re-check for API key in case it was set externally\n",
        "    api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "    if not api_key or api_key == \"YOUR_GOOGLE_API_KEY\":\n",
        "         # Raising an error here will be caught by the except block below\n",
        "        raise ValueError(\"GOOGLE_API_KEY environment variable not set or is a placeholder.\")\n",
        "\n",
        "    # Configure the model. This might fail if the key is invalid or quota is exceeded.\n",
        "    # Use the evaluation_model variable if it was already configured successfully\n",
        "    if 'evaluation_model' not in locals() or evaluation_model is None:\n",
        "        genai.configure(api_key=api_key)\n",
        "        evaluation_model = genai.GenerativeModel('gemini-1.5-flash') # Or choose a different model\n",
        "        print(\"Google Generative AI configured successfully for feedback generation.\")\n",
        "    else:\n",
        "        print(\"Google Generative AI was already configured for feedback generation.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to configure Google Generative AI for feedback generation: {e}\")\n",
        "    evaluation_model = None # Ensure model is None if configuration fails\n",
        "\n",
        "\n",
        "def generate_feedback_rag(evaluation, user_answer, question, role, model):\n",
        "    \"\"\"\n",
        "    Generates friendly and constructive feedback based on the RAG-informed evaluation.\n",
        "\n",
        "    Args:\n",
        "        evaluation: The structured evaluation string from the evaluate_answer_rag function.\n",
        "        user_answer: The user's original answer to the question.\n",
        "        question: The question that was asked.\n",
        "        role: The role for which the interview is being conducted (e.g., \"Mean Stack\").\n",
        "        model: The generative AI model to use for generating feedback.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the friendly and constructive feedback, or an error message.\n",
        "    \"\"\"\n",
        "    # 7. Implement error handling\n",
        "    if model is None:\n",
        "        return \"Feedback generation model not configured. Cannot generate feedback.\"\n",
        "\n",
        "    # 2. Construct a detailed prompt for the generative AI model\n",
        "    prompt = f\"\"\"\n",
        "    You are a friendly, encouraging, and knowledgeable AI interviewer providing feedback to a candidate for a {role} role.\n",
        "    Based on the following structured evaluation (which is informed by relevant knowledge) and the candidate's answer to the question,\n",
        "    synthesize the information into constructive and actionable feedback.\n",
        "    Maintain a supportive and encouraging tone throughout the feedback.\n",
        "\n",
        "    **Interview Question:**\n",
        "    {question}\n",
        "\n",
        "    **Candidate's Answer:**\n",
        "    {user_answer}\n",
        "\n",
        "    **Structured Evaluation:**\n",
        "    {evaluation}\n",
        "\n",
        "    **Constructive Feedback:**\n",
        "    \"\"\"\n",
        "\n",
        "    # 5. Call the generative AI model's content generation method\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        # 7. Implement error handling\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred during feedback generation: {e}\"\n",
        "\n",
        "# 8. Demonstrate the generate_feedback_rag function if the model is configured\n",
        "print(\"\\n--- Demonstrating generate_feedback_rag ---\")\n",
        "\n",
        "if 'evaluation_model' in locals() and evaluation_model is not None:\n",
        "    # 8. Demonstrate with a sample structured evaluation (manually created)\n",
        "    # This sample evaluation mimics the expected output of evaluate_answer_rag\n",
        "    sample_question_fb = \"Explain the concept of overfitting and how to prevent it in machine learning.\"\n",
        "    sample_user_answer_fb = \"Overfitting is when a model performs well on training data but poorly on new data. You can prevent it by using more data, regularization, or cross-validation.\"\n",
        "    sample_role_fb = \"AI\"\n",
        "    # Sample evaluation mimicking RAG-informed output\n",
        "    sample_structured_evaluation = f\"\"\"\n",
        "    Evaluation for {sample_role_fb} Role:\n",
        "\n",
        "    **Knowledge Base Information Used:**\n",
        "    - Overfitting: When a model learns the training data too well, including noise, and performs poorly on unseen data.\n",
        "    - Regularization techniques (L1, L2, dropout) help prevent overfitting.\n",
        "    - Cross-validation is a technique to evaluate model performance and detect overfitting.\n",
        "\n",
        "    **Correctness (Technical Accuracy):** Good (4/5). The core definition of overfitting is correct. The mentioned prevention methods (more data, regularization, cross-validation) are valid. Could be slightly more precise about *why* more data helps.\n",
        "    **Clarity (Ease of Understanding):** Excellent (5/5). The explanation is clear and easy to follow.\n",
        "    **Completeness (Coverage of Relevant Aspects):** Good (4/5). The answer covers the main concept and common prevention methods. Mentioning specific types of regularization (L1, L2, dropout) would make it more complete, drawing from the knowledge base.\n",
        "\n",
        "    Summary: A good foundational answer.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Generating feedback for sample answer on '{sample_question_fb}' in {sample_role_fb} role.\")\n",
        "\n",
        "    generated_feedback_rag = generate_feedback_rag(\n",
        "        sample_structured_evaluation,\n",
        "        sample_user_answer_fb,\n",
        "        sample_question_fb,\n",
        "        sample_role_fb,\n",
        "        evaluation_model\n",
        "    )\n",
        "\n",
        "    # 9. Print the generated feedback\n",
        "    print(\"\\nGenerated Feedback (RAG-informed):\")\n",
        "    print(generated_feedback_rag)\n",
        "\n",
        "else:\n",
        "    print(\"\\nEvaluation model not available, skipping generate_feedback_rag demonstration.\")\n",
        "    print(\"Please ensure your GOOGLE_API_KEY is set correctly and you have sufficient quota.\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to configure Google Generative AI for feedback generation: GOOGLE_API_KEY environment variable not set or is a placeholder.\n",
            "\n",
            "--- Demonstrating generate_feedback_rag ---\n",
            "\n",
            "Evaluation model not available, skipping generate_feedback_rag demonstration.\n",
            "Please ensure your GOOGLE_API_KEY is set correctly and you have sufficient quota.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb24c221"
      },
      "source": [
        "## Provide feedback (with rag and prompt engineering) retry 1\n",
        "\n",
        "### Subtask:\n",
        "Generate constructive and friendly feedback based on the evaluation, using prompt engineering and incorporating insights from the RAG-based evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c98a912"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to generate feedback failed because the generative AI model could not be configured due to an invalid or missing API key. The subtask requires generating feedback using the model. Therefore, the next step is to retry configuring the model and executing the feedback generation code, assuming the user has now set a valid `GOOGLE_API_KEY` environment variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de4e1666",
        "outputId": "9d86afa9-ebc1-4451-8f89-5ee837dc8097"
      },
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "print(\"Retrying feedback generation with RAG and prompt engineering.\")\n",
        "\n",
        "# Ensure the generative AI model for feedback generation is configured\n",
        "# Re-checking configuration in case environment changed\n",
        "try:\n",
        "    # Re-check for API key in case it was set externally\n",
        "    api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "    if not api_key or api_key == \"YOUR_GOOGLE_API_KEY\":\n",
        "         # Raising an error here will be caught by the except block below\n",
        "        raise ValueError(\"GOOGLE_API_KEY environment variable not set or is a placeholder.\")\n",
        "\n",
        "    # Configure the model. This might fail if the key is invalid or quota is exceeded.\n",
        "    # Use the evaluation_model variable if it was already configured successfully\n",
        "    if 'evaluation_model' not in locals() or evaluation_model is None:\n",
        "        genai.configure(api_key=api_key)\n",
        "        evaluation_model = genai.GenerativeModel('gemini-1.5-flash') # Or choose a different model\n",
        "        print(\"Google Generative AI configured successfully for feedback generation.\")\n",
        "    else:\n",
        "        print(\"Google Generative AI was already configured for feedback generation.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to configure Google Generative AI for feedback generation: {e}\")\n",
        "    evaluation_model = None # Ensure model is None if configuration fails\n",
        "\n",
        "\n",
        "# The generate_feedback_rag function remains the same as defined previously,\n",
        "# it will use the evaluation, user_answer, question, role, and the configured model.\n",
        "# Assuming the previous cell where generate_feedback_rag was defined was executed successfully.\n",
        "\n",
        "# Demonstrate the generate_feedback_rag function if the model is configured\n",
        "print(\"\\n--- Demonstrating generate_feedback_rag ---\")\n",
        "\n",
        "if 'evaluation_model' in locals() and evaluation_model is not None:\n",
        "    # Demonstrate with a sample structured evaluation (manually created)\n",
        "    # This sample evaluation mimics the expected output of evaluate_answer_rag\n",
        "    sample_question_fb = \"Explain the concept of overfitting and how to prevent it in machine learning.\"\n",
        "    sample_user_answer_fb = \"Overfitting is when a model performs well on training data but poorly on new data. You can prevent it by using more data, regularization, or cross-validation.\"\n",
        "    sample_role_fb = \"AI\"\n",
        "    # Sample evaluation mimicking RAG-informed output\n",
        "    sample_structured_evaluation = f\"\"\"\n",
        "    Evaluation for {sample_role_fb} Role:\n",
        "\n",
        "    **Knowledge Base Information Used:**\n",
        "    - Overfitting: When a model learns the training data too well, including noise, and performs poorly on unseen data.\n",
        "    - Regularization techniques (L1, L2, dropout) help prevent overfitting.\n",
        "    - Cross-validation is a technique to evaluate model performance and detect overfitting.\n",
        "\n",
        "    **Correctness (Technical Accuracy):** Good (4/5). The core definition of overfitting is correct. The mentioned prevention methods (more data, regularization, cross-validation) are valid. Could be slightly more precise about *why* more data helps.\n",
        "    **Clarity (Ease of Understanding):** Excellent (5/5). The explanation is clear and easy to follow.\n",
        "    **Completeness (Coverage of Relevant Aspects):** Good (4/5). The answer covers the main concept and common prevention methods. Mentioning specific types of regularization (L1, L2, dropout) would make it more complete, drawing from the knowledge base.\n",
        "\n",
        "    Summary: A good foundational answer.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Generating feedback for sample answer on '{sample_question_fb}' in {sample_role_fb} role.\")\n",
        "\n",
        "    generated_feedback_rag = generate_feedback_rag(\n",
        "        sample_structured_evaluation,\n",
        "        sample_user_answer_fb,\n",
        "        sample_question_fb,\n",
        "        sample_role_fb,\n",
        "        evaluation_model\n",
        "    )\n",
        "\n",
        "    # Print the generated feedback\n",
        "    print(\"\\nGenerated Feedback (RAG-informed):\")\n",
        "    print(generated_feedback_rag)\n",
        "\n",
        "else:\n",
        "    print(\"\\nEvaluation model not available, skipping generate_feedback_rag demonstration.\")\n",
        "    print(\"Please ensure your GOOGLE_API_KEY is set correctly and you have sufficient quota.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrying feedback generation with RAG and prompt engineering.\n",
            "Failed to configure Google Generative AI for feedback generation: GOOGLE_API_KEY environment variable not set or is a placeholder.\n",
            "\n",
            "--- Demonstrating generate_feedback_rag ---\n",
            "\n",
            "Evaluation model not available, skipping generate_feedback_rag demonstration.\n",
            "Please ensure your GOOGLE_API_KEY is set correctly and you have sufficient quota.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08250b13"
      },
      "source": [
        "## Iterate and improve\n",
        "\n",
        "### Subtask:\n",
        "Continuously refine the questions, evaluation criteria, and feedback mechanism based on user interaction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2199765f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous analysis provided a basic summary of the collected feedback. To refine the process, I need to process the collected feedback data more thoroughly to identify specific trends in ratings and common themes in comments. This involves calculating the average rating, counting the occurrences of keywords in comments, and potentially categorizing comments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be516541",
        "outputId": "a80e83c2-69cb-409d-ce7f-dca603d2e52f"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Ensure the user_feedback_data is available. Assuming it was populated in a previous step.\n",
        "# If not, re-initialize with sample data for analysis demonstration.\n",
        "if 'user_feedback_data' not in locals() or not user_feedback_data:\n",
        "    print(\"user_feedback_data not found or empty. Using sample data for analysis.\")\n",
        "    user_feedback_data = []\n",
        "    from datetime import datetime\n",
        "    def collect_feedback(interview_id, question, user_answer, generated_feedback, user_rating, user_comments):\n",
        "        feedback_entry = {\n",
        "            \"interview_id\": interview_id,\n",
        "            \"question\": question,\n",
        "            \"user_answer\": user_answer,\n",
        "            \"generated_feedback\": generated_feedback,\n",
        "            \"user_rating\": user_rating,\n",
        "            \"user_comments\": user_comments,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        user_feedback_data.append(feedback_entry)\n",
        "\n",
        "    collect_feedback(\n",
        "        interview_id=\"interview_demo_1\",\n",
        "        question=\"Explain hoisting in JavaScript.\",\n",
        "        user_answer=\"Hoisting moves variable and function declarations to the top of their scope.\",\n",
        "        generated_feedback=\"Good explanation of hoisting. Mentioning the difference between var, let, and const would improve completeness.\",\n",
        "        user_rating=5,\n",
        "        user_comments=\"Clear and concise feedback.\"\n",
        "    )\n",
        "\n",
        "    collect_feedback(\n",
        "        interview_id=\"interview_demo_1\",\n",
        "        question=\"What are React Hooks?\",\n",
        "        user_answer=\"Hooks are functions that let you use state and other React features without writing a class.\",\n",
        "        generated_feedback=\"Correct definition. Elaborating on specific hooks like useState or useEffect would be beneficial.\",\n",
        "        user_rating=4,\n",
        "        user_comments=\"Helpful, but could provide examples.\"\n",
        "    )\n",
        "\n",
        "    collect_feedback(\n",
        "        interview_id=\"interview_demo_2\",\n",
        "        question=\"Describe gradient descent.\",\n",
        "        user_answer=\"It's an optimization algorithm to find the minimum of a function.\",\n",
        "        generated_feedback=\"Basic definition is correct. Explaining the steps (learning rate, iterations) and types (batch, stochastic) is crucial for a complete answer.\",\n",
        "        user_rating=3,\n",
        "        user_comments=\"Feedback was a bit too general.\"\n",
        "    )\n",
        "    collect_feedback(\n",
        "        interview_id=\"interview_demo_3\",\n",
        "        question=\"What is a closure in JavaScript?\",\n",
        "        user_answer=\"A closure is a function that remembers the variables from its outer scope.\",\n",
        "        generated_feedback=\"Correct definition. Providing a simple code example would enhance understanding.\",\n",
        "        user_rating=4,\n",
        "        user_comments=\"Example would be great.\"\n",
        "    )\n",
        "    collect_feedback(\n",
        "        interview_id=\"interview_demo_4\",\n",
        "        question=\"Explain the difference between `==` and `===` in JavaScript.\",\n",
        "        user_answer=\"`==` checks value, `===` checks value and type.\",\n",
        "        generated_feedback=\"Concise and correct. Mentioning type coercion for `==` would improve completeness.\",\n",
        "        user_rating=5,\n",
        "        user_comments=\"Perfect feedback.\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Convert feedback data to a pandas DataFrame for easier analysis\n",
        "feedback_df = pd.DataFrame(user_feedback_data)\n",
        "\n",
        "# Analyze trends in user ratings\n",
        "average_rating = feedback_df['user_rating'].mean()\n",
        "rating_distribution = feedback_df['user_rating'].value_counts(normalize=True).sort_index() * 100\n",
        "\n",
        "print(\"\\n--- Feedback Analysis ---\")\n",
        "print(f\"\\nAverage User Rating: {average_rating:.2f}\")\n",
        "print(\"\\nUser Rating Distribution (%):\")\n",
        "print(rating_distribution)\n",
        "\n",
        "# Analyze comments for common themes (basic keyword analysis)\n",
        "def analyze_comments(comments):\n",
        "    \"\"\"Basic keyword analysis of comments.\"\"\"\n",
        "    themes = {\n",
        "        \"clarity\": 0,\n",
        "        \"specificity\": 0,\n",
        "        \"examples\": 0,\n",
        "        \"general\": 0,\n",
        "        \"helpful\": 0,\n",
        "        \"correctness\": 0,\n",
        "        \"completeness\": 0,\n",
        "        \"tone\": 0,\n",
        "        \"too short\": 0,\n",
        "        \"too long\": 0,\n",
        "        \"great\": 0,\n",
        "        \"good\": 0,\n",
        "        \"needs improvement\": 0,\n",
        "        \"more\": 0 # generic keyword for requesting more detail\n",
        "    }\n",
        "    for comment in comments:\n",
        "        if isinstance(comment, str): # Ensure comment is a string\n",
        "            comment_lower = comment.lower()\n",
        "            for keyword in themes.keys():\n",
        "                if keyword in comment_lower:\n",
        "                    themes[keyword] += 1\n",
        "    return themes\n",
        "\n",
        "comment_themes = analyze_comments(feedback_df['user_comments'])\n",
        "\n",
        "print(\"\\nCommon Themes in Comments (Keyword Count):\")\n",
        "# Filter out themes with zero occurrences for cleaner output\n",
        "present_themes = {k: v for k, v in comment_themes.items() if v > 0}\n",
        "for theme, count in sorted(present_themes.items(), key=lambda item: item[1], reverse=True):\n",
        "     print(f\"- {theme.capitalize()}: {count}\")\n",
        "\n",
        "# Display comments for manual review\n",
        "print(\"\\nAll User Comments:\")\n",
        "for i, comment in enumerate(feedback_df['user_comments']):\n",
        "    print(f\"{i+1}. {comment}\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_feedback_data not found or empty. Using sample data for analysis.\n",
            "\n",
            "--- Feedback Analysis ---\n",
            "\n",
            "Average User Rating: 4.20\n",
            "\n",
            "User Rating Distribution (%):\n",
            "user_rating\n",
            "3    20.0\n",
            "4    40.0\n",
            "5    40.0\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Common Themes in Comments (Keyword Count):\n",
            "- Examples: 1\n",
            "- General: 1\n",
            "- Helpful: 1\n",
            "- Great: 1\n",
            "\n",
            "All User Comments:\n",
            "1. Clear and concise feedback.\n",
            "2. Helpful, but could provide examples.\n",
            "3. Feedback was a bit too general.\n",
            "4. Example would be great.\n",
            "5. Perfect feedback.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dce6e301"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the feedback analysis, I need to summarize the key areas for improvement and propose specific adjustments to the RAG knowledge base, prompt engineering, or interview flow. This involves interpreting the average rating, rating distribution, and identified comment themes to formulate actionable recommendations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33f8e77a",
        "outputId": "7ab86805-4aba-4a21-e386-104110862100"
      },
      "source": [
        "# Summarize analysis and propose improvements\n",
        "\n",
        "print(\"\\n--- Analysis Summary and Proposed Improvements ---\")\n",
        "\n",
        "# 1. Summarize key findings from the analysis\n",
        "print(\"\\nKey Findings from Feedback Analysis:\")\n",
        "print(f\"- Average User Rating: {average_rating:.2f} (out of 5)\")\n",
        "print(f\"- Rating Distribution:\\n{rating_distribution.to_string()}\")\n",
        "print(\"\\nIdentified Comment Themes (based on keyword analysis):\")\n",
        "present_themes = {k: v for k, v in comment_themes.items() if v > 0} # Re-use present_themes from analysis\n",
        "if present_themes:\n",
        "    for theme, count in sorted(present_themes.items(), key=lambda item: item[1], reverse=True):\n",
        "         print(f\"  - {theme.capitalize()}: {count} mentions\")\n",
        "else:\n",
        "    print(\"  - No specific themes identified from keyword analysis.\")\n",
        "\n",
        "# Interpret findings and identify areas for improvement\n",
        "print(\"\\nInterpretation and Areas for Improvement:\")\n",
        "if average_rating >= 4.0:\n",
        "    print(\"- Overall, the feedback is positive, with an average rating above 4.0.\")\n",
        "elif average_rating >= 3.0:\n",
        "     print(\"- The feedback is generally satisfactory, but there's room for improvement.\")\n",
        "else:\n",
        "    print(\"- The feedback suggests significant areas require attention to improve user experience.\")\n",
        "\n",
        "if 'examples' in present_themes and present_themes['examples'] > 0:\n",
        "    print(\"- A recurring theme is the request for more examples, particularly in feedback.\")\n",
        "if 'general' in present_themes and present_themes['general'] > 0:\n",
        "     print(\"- Some feedback is perceived as too general.\")\n",
        "if 'specificity' in present_themes and present_themes['specificity'] > 0:\n",
        "    print(\"- Users are asking for more specific feedback.\")\n",
        "if 'completeness' in comment_themes and comment_themes['completeness'] > 0: # Use comment_themes here as it includes zero counts\n",
        "     print(\"- There are comments related to the completeness of answers or feedback.\")\n",
        "if 'clarity' in comment_themes and comment_themes['clarity'] > 0:\n",
        "     print(\"- Clarity of feedback was mentioned.\")\n",
        "\n",
        "\n",
        "# 2. Propose specific adjustments based on the analysis\n",
        "print(\"\\nProposed Adjustments:\")\n",
        "\n",
        "print(\"\\nAdjustments to Prompt Engineering:\")\n",
        "print(\"- **For `generate_feedback_rag`:** Modify the prompt to explicitly instruct the model to provide concrete examples or code snippets where relevant, especially for technical questions. Emphasize the need for specific and actionable feedback.\")\n",
        "print(\"- **For `evaluate_answer_rag`:** Refine the prompt to encourage the model to identify missing details or alternative approaches based on the RAG context, which can then be used in the feedback.\")\n",
        "print(\"- **For `generate_interview_questions_rag`:** Consider including instructions to generate questions that might require or lend themselves well to answers with examples.\")\n",
        "\n",
        "\n",
        "print(\"\\nAdjustments to RAG Knowledge Base:\")\n",
        "print(\"- **Content Enrichment:** Add more specific examples, code snippets, and detailed explanations to the `rag_knowledge_base` for key concepts and common interview topics across all roles.\")\n",
        "print(\"- **Granularity:** Potentially break down some broader knowledge base entries into more specific points to allow for more targeted retrieval during evaluation and feedback generation.\")\n",
        "\n",
        "print(\"\\nAdjustments to Interview Flow:\")\n",
        "print(\"- **Post-Feedback Prompt:** After providing feedback, include an optional step where the user can ask for clarification or further examples on the feedback provided.\")\n",
        "print(\"- **Rating Granularity:** If necessary, refine the feedback rating system to be more granular or include specific categories for feedback quality (e.g., clarity rating, helpfulness rating) to collect more targeted improvement data.\")\n",
        "\n",
        "\n",
        "print(\"\\nIterative Improvement Process:\")\n",
        "print(\"- Continuously collect user feedback using the `collect_feedback` function.\")\n",
        "print(\"- Periodically re-run the `analyze_feedback` function to identify new trends or changes in user sentiment.\")\n",
        "print(\"- Use the analysis summaries to inform further refinements to the RAG knowledge base and prompt engineering.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Analysis Summary and Proposed Improvements ---\n",
            "\n",
            "Key Findings from Feedback Analysis:\n",
            "- Average User Rating: 4.20 (out of 5)\n",
            "- Rating Distribution:\n",
            "user_rating\n",
            "3    20.0\n",
            "4    40.0\n",
            "5    40.0\n",
            "\n",
            "Identified Comment Themes (based on keyword analysis):\n",
            "  - Examples: 1 mentions\n",
            "  - General: 1 mentions\n",
            "  - Helpful: 1 mentions\n",
            "  - Great: 1 mentions\n",
            "\n",
            "Interpretation and Areas for Improvement:\n",
            "- Overall, the feedback is positive, with an average rating above 4.0.\n",
            "- A recurring theme is the request for more examples, particularly in feedback.\n",
            "- Some feedback is perceived as too general.\n",
            "\n",
            "Proposed Adjustments:\n",
            "\n",
            "Adjustments to Prompt Engineering:\n",
            "- **For `generate_feedback_rag`:** Modify the prompt to explicitly instruct the model to provide concrete examples or code snippets where relevant, especially for technical questions. Emphasize the need for specific and actionable feedback.\n",
            "- **For `evaluate_answer_rag`:** Refine the prompt to encourage the model to identify missing details or alternative approaches based on the RAG context, which can then be used in the feedback.\n",
            "- **For `generate_interview_questions_rag`:** Consider including instructions to generate questions that might require or lend themselves well to answers with examples.\n",
            "\n",
            "Adjustments to RAG Knowledge Base:\n",
            "- **Content Enrichment:** Add more specific examples, code snippets, and detailed explanations to the `rag_knowledge_base` for key concepts and common interview topics across all roles.\n",
            "- **Granularity:** Potentially break down some broader knowledge base entries into more specific points to allow for more targeted retrieval during evaluation and feedback generation.\n",
            "\n",
            "Adjustments to Interview Flow:\n",
            "- **Post-Feedback Prompt:** After providing feedback, include an optional step where the user can ask for clarification or further examples on the feedback provided.\n",
            "- **Rating Granularity:** If necessary, refine the feedback rating system to be more granular or include specific categories for feedback quality (e.g., clarity rating, helpfulness rating) to collect more targeted improvement data.\n",
            "\n",
            "Iterative Improvement Process:\n",
            "- Continuously collect user feedback using the `collect_feedback` function.\n",
            "- Periodically re-run the `analyze_feedback` function to identify new trends or changes in user sentiment.\n",
            "- Use the analysis summaries to inform further refinements to the RAG knowledge base and prompt engineering.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fbd3b9a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   A structured knowledge base was successfully created using a Python dictionary for Mean Stack, MERN Stack, and AI roles, containing foundational concepts and common interview questions.\n",
        "*   A basic Retrieval Augmented Generation (RAG) retrieval mechanism was implemented using keyword matching and substring checks to fetch relevant information from the knowledge base.\n",
        "*   Attempts to generate interview questions and evaluate user responses using the Google Generative AI model failed repeatedly due to the `GOOGLE_API_KEY` environment variable not being set, preventing model configuration and API calls.\n",
        "*   Despite the API issues, the structure for simulating an interview flow was established, allowing for questions to be presented and user input to be collected (demonstrated using placeholder questions).\n",
        "*   Analysis of sample user feedback data revealed a generally positive average rating (4.20/5) but highlighted user requests for more specific feedback, concrete examples, and less generalized comments.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The primary blocker for completing the task is the unconfigured Google Generative AI model due to the missing or invalid `GOOGLE_API_KEY`. **The immediate next step is for the user to set a valid `GOOGLE_API_KEY` environment variable with sufficient quota to enable the AI model functionality.**\n",
        "*   Once the API is functional, the next steps should focus on integrating the generated questions, RAG-informed evaluation, and feedback generation into the simulated interview flow. Further refinement of prompts and the RAG knowledge base should be guided by continuous user feedback analysis as initially planned.\n"
      ]
    }
  ]
}